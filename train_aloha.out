Sun Jan  4 03:14:04 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   24C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   23C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   23C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   29C    P0             73W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
0,1,2,3,4,5,6,7
warning: The `tool.uv.dev-dependencies` field (used in `packages/openpi-client/pyproject.toml`) is deprecated and will be removed in a future release; use `dependency-groups.dev` instead
03:14:38.944 [I] Running on: dgx-03                                                               (394891:train.py:196)
INFO:2026-01-04 03:14:41,003:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
03:14:41.003 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (394891:xla_bridge.py:925)
INFO:2026-01-04 03:14:41,011:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
03:14:41.011 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (394891:xla_bridge.py:925)
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/scripts/train.py", line 280, in <module>
    main(_config.cli())
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/scripts/train.py", line 208, in main
    mesh = sharding.make_mesh(config.fsdp_devices)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/training/sharding.py", line 19, in make_mesh
    raise ValueError(
ValueError: Number of devices 4 must be divisible by the number of FSDP devices 8.
Sun Jan  4 14:13:04 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   23C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   25C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   25C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   23C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   21C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   22C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   23C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   22C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
0,1,2,3,4,5,6,7
warning: The `tool.uv.dev-dependencies` field (used in `packages/openpi-client/pyproject.toml`) is deprecated and will be removed in a future release; use `dependency-groups.dev` instead
14:14:47.718 [I] Running on: dgx-05                                                               (1523236:train.py:196)
INFO:2026-01-04 14:14:55,815:jax._src.xla_bridge:925: Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'
14:14:55.815 [I] Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig' (1523236:xla_bridge.py:925)
INFO:2026-01-04 14:14:56,017:jax._src.xla_bridge:925: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
14:14:56.017 [I] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory (1523236:xla_bridge.py:925)
14:15:05.466 [I] Wiped checkpoint directory /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes (1523236:checkpoints.py:29)
14:15:05.467 [I] Created BasePyTreeCheckpointHandler: use_ocdbt=True, use_zarr3=False, pytree_metadata_options=PyTreeMetadataOptions(support_rich_types=False), array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x1554b5977550> (1523236:base_pytree_checkpoint_handler.py:334)
14:15:05.467 [I] Created BasePyTreeCheckpointHandler: use_ocdbt=True, use_zarr3=False, pytree_metadata_options=PyTreeMetadataOptions(support_rich_types=False), array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x1554b5977550> (1523236:base_pytree_checkpoint_handler.py:334)
14:15:05.467 [I] [thread=MainThread] Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID. (1523236:multihost.py:390)
14:15:05.468 [I] [process=0][thread=MainThread] CheckpointManager init: checkpointers=None, item_names=None, item_handlers={'assets': <openpi.training.checkpoints.CallbackHandler object at 0x15532bd84950>, 'train_state': <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15532be145d0>, 'params': <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15532be15b10>}, handler_registry=None (1523236:checkpoint_manager.py:620)
14:15:05.468 [I] Deferred registration for item: "assets". Adding handler `<openpi.training.checkpoints.CallbackHandler object at 0x15532bd84950>` for item "assets" and save args `<class 'openpi.training.checkpoints.CallbackSave'>` and restore args `<class 'openpi.training.checkpoints.CallbackRestore'>` to `_handler_registry`. (1523236:composite_checkpoint_handler.py:234)
14:15:05.468 [I] Deferred registration for item: "train_state". Adding handler `<orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15532be145d0>` for item "train_state" and save args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>` to `_handler_registry`. (1523236:composite_checkpoint_handler.py:234)
14:15:05.468 [I] Deferred registration for item: "params". Adding handler `<orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15532be15b10>` for item "params" and save args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>` to `_handler_registry`. (1523236:composite_checkpoint_handler.py:234)
14:15:05.468 [I] Deferred registration for item: "metrics". Adding handler `<orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x15532be16010>` for item "metrics" and save args `<class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonRestoreArgs'>` to `_handler_registry`. (1523236:composite_checkpoint_handler.py:234)
14:15:05.468 [I] Initialized registry DefaultCheckpointHandlerRegistry({('assets', <class 'openpi.training.checkpoints.CallbackSave'>): <openpi.training.checkpoints.CallbackHandler object at 0x15532bd84950>, ('assets', <class 'openpi.training.checkpoints.CallbackRestore'>): <openpi.training.checkpoints.CallbackHandler object at 0x15532bd84950>, ('train_state', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15532be145d0>, ('train_state', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15532be145d0>, ('params', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15532be15b10>, ('params', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x15532be15b10>, ('metrics', <class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonSaveArgs'>): <orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x15532be16010>, ('metrics', <class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonRestoreArgs'>): <orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x15532be16010>}). (1523236:composite_checkpoint_handler.py:502)
14:15:05.468 [I] orbax-checkpoint version: 0.11.13                                                (1523236:abstract_checkpointer.py:35)
14:15:05.468 [I] [process=0][thread=MainThread] Using barrier_sync_fn: <function get_barrier_sync_fn.<locals>.<lambda> at 0x1553180832e0> timeout: 7200 secs and primary_host=0 for async checkpoint writes (1523236:async_checkpointer.py:170)
14:15:05.470 [I] Found 0 checkpoint steps in /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes (1523236:checkpoint_manager.py:1701)
14:15:05.470 [I] [process=0][thread=MainThread] CheckpointManager created,  primary_host=0, CheckpointManagerOptions=CheckpointManagerOptions(save_interval_steps=1, max_to_keep=1, keep_time_interval=None, keep_period=5000, should_keep_fn=None, best_fn=None, best_mode='max', keep_checkpoints_without_metrics=True, step_prefix=None, step_format_fixed_length=None, step_name_format=None, create=False, cleanup_tmp_directories=False, save_on_steps=frozenset(), single_host_load_and_broadcast=False, todelete_subdir=None, enable_background_delete=False, read_only=False, enable_async_checkpointing=True, async_options=AsyncOptions(timeout_secs=7200, barrier_sync_fn=None, post_finalization_callback=None, create_directories_asynchronously=True), multiprocessing_options=MultiprocessingOptions(primary_host=0, active_processes=None, barrier_sync_key_prefix=None), should_save_fn=None, file_options=FileOptions(path_permission_mode=None), save_root_metadata=True, temporary_path_class=None, save_decision_policy=None, prevent_write_metrics=False), root_directory=/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes: <orbax.checkpoint.checkpoint_manager.CheckpointManager object at 0x15532bb385d0> (1523236:checkpoint_manager.py:801)
wandb: Currently logged in as: heisen0928 (heisen0928-the-hong-kong-polytechnic-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/wandb/run-20260104_141509-x3eius4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run folding_clothes
wandb: ‚≠êÔ∏è View project at https://wandb.ai/heisen0928-the-hong-kong-polytechnic-university/openpi
wandb: üöÄ View run at https://wandb.ai/heisen0928-the-hong-kong-polytechnic-university/openpi/runs/x3eius4s
14:15:11.777 [I] Loaded norm stats from /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/assets/pi0_base_aloha_robotwin_full/folding_clothes (1523236:config.py:196)
14:15:11.778 [I] data_config: DataConfig(repo_id='folding_clothes', asset_id='folding_clothes', norm_stats={'state': NormStats(mean=array([-0.17993796,  1.67902672, -1.27645063, -0.25658029,  0.82060319,
        0.25432473,  0.01876085,  0.16258715,  1.61566854, -1.19282651,
        0.18029477,  0.82395399, -0.08512198,  0.02922819]), std=array([0.22838135, 0.41188484, 0.64074528, 0.24707064, 0.22979081,
       0.21594717, 0.02282616, 0.23225886, 0.40123916, 0.5209471 ,
       0.28343889, 0.20789632, 0.22671698, 0.02396917]), q01=array([-9.10893666e-01,  8.41906461e-01, -2.43922062e+00, -8.46724608e-01,
        2.45584981e-01, -2.99573876e-01, -1.55310395e-03, -4.29880006e-01,
        8.36437613e-01, -2.44185737e+00, -4.91898922e-01,  2.56816711e-01,
       -7.85465667e-01, -1.61000004e-03]), q99=array([ 0.25616421,  2.51955448, -0.50947183,  0.42924743,  1.22396753,
        0.76045431,  0.06929871,  0.85175634,  2.52753692, -0.47760451,
        1.14052764,  1.2202494 ,  0.50120836,  0.06936335])), 'actions': NormStats(mean=array([-0.00516585, -0.00273914,  0.00483681, -0.00113156, -0.00429881,
        0.00132648,  0.01744885,  0.00447215,  0.0004924 ,  0.00022902,
       -0.00155421, -0.00134348,  0.00172754,  0.02821313]), std=array([0.16302365, 0.32316741, 0.74333346, 0.18957819, 0.19137973,
       0.14140716, 0.02374665, 0.19732474, 0.34592336, 0.39529315,
       0.21691571, 0.17495938, 0.16368915, 0.02552901]), q01=array([-0.62728671, -1.02477976, -1.2820312 , -0.62337801, -0.57002299,
       -0.43457336, -0.0025184 , -0.64948253, -0.95135734, -1.14287382,
       -0.73286652, -0.50019678, -0.51601629, -0.0026008 ]), q99=array([0.51060519, 0.88477528, 1.2820312 , 0.55245526, 0.51162245,
       0.4404316 , 0.0698803 , 0.68321772, 1.04521629, 1.35989635,
       0.6476214 , 0.48795937, 0.56588434, 0.0708856 ]))}, repack_transforms=Group(inputs=[RepackTransform(structure={'images': {'cam_high': 'observation.images.cam_high', 'cam_left_wrist': 'observation.images.cam_left_wrist', 'cam_right_wrist': 'observation.images.cam_right_wrist'}, 'state': 'observation.state', 'actions': 'action', 'prompt': 'prompt'})], outputs=()), data_transforms=Group(inputs=(AlohaInputs(adapt_to_pi=False), DeltaActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False))), outputs=(AbsoluteActions(mask=(True, True, True, True, True, True, False, True, True, True, True, True, True, False)), AlohaOutputs(adapt_to_pi=False))), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), ResizeImages(height=224, width=224), TokenizePrompt(tokenizer=<openpi.models.tokenizer.PaligemmaTokenizer object at 0x1554bdfbc9d0>, discrete_state_input=False), PadStatesAndActions(model_action_dim=32)], outputs=()), use_quantile_norm=False, action_sequence_keys=('action',), prompt_from_task=True, rlds_data_dir=None, action_space=None, filter_dict_path=None) (1523236:data_loader.py:243)
14:15:18.427 [I] local_batch_size: 128                                                            (1523236:data_loader.py:324)
14:15:50.547 [I] Initialized data loader:
[0].images['base_0_rgb']: (128, 224, 224, 3)@float32
[0].images['left_wrist_0_rgb']: (128, 224, 224, 3)@float32
[0].images['right_wrist_0_rgb']: (128, 224, 224, 3)@float32
[0].image_masks['base_0_rgb']: (128,)@bool
[0].image_masks['left_wrist_0_rgb']: (128,)@bool
[0].image_masks['right_wrist_0_rgb']: (128,)@bool
[0].state: (128, 32)@float32
[0].tokenized_prompt: (128, 48)@int32
[0].tokenized_prompt_mask: (128, 48)@bool
[1]: (128, 50, 32)@float32 (1523236:train.py:227)
14:15:54.609 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value of shape (27, 1152, 4304) (510.68 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.609 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value of shape (27, 4304, 1152) (510.68 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.609 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.609 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value of shape (27, 16, 72, 1152) (136.69 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['img']['head']['kernel'].value of shape (1152, 2048) (9.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['embedder']['input_embedding'].value of shape (257152, 2048) (2009.00 MiB) along axis 0 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value of shape (18, 8, 256, 2048) (288.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value of shape (18, 8, 256, 1024) (144.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value of shape (18, 2, 1, 2048, 256) (72.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value of shape (18, 2, 1, 1024, 256) (36.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value of shape (18, 8, 2048, 256) (288.00 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value of shape (18, 8, 1024, 256) (144.00 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value of shape (18, 2, 2048, 16384) (4608.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['mlp']['linear'].value of shape (18, 16384, 2048) (2304.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value of shape (18, 2, 1024, 4096) (576.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.610 [I] Sharding .params['PaliGemma']['llm']['layers']['mlp_1']['linear'].value of shape (18, 4096, 1024) (288.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .params['action_time_mlp_in']['kernel'].value of shape (2048, 1024) (8.00 MiB) along axis 0 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .params['action_time_mlp_out']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value of shape (27, 1152, 4304) (510.68 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value of shape (27, 4304, 1152) (510.68 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value of shape (27, 16, 72, 1152) (136.69 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['head']['kernel'].value of shape (1152, 2048) (9.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['embedder']['input_embedding'].value of shape (257152, 2048) (2009.00 MiB) along axis 0 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value of shape (18, 8, 256, 2048) (288.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value of shape (18, 8, 256, 1024) (144.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.611 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value of shape (18, 2, 1, 2048, 256) (72.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value of shape (18, 2, 1, 1024, 256) (36.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value of shape (18, 8, 2048, 256) (288.00 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value of shape (18, 8, 1024, 256) (144.00 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value of shape (18, 2, 2048, 16384) (4608.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['mlp']['linear'].value of shape (18, 16384, 2048) (2304.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value of shape (18, 2, 1024, 4096) (576.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['mlp_1']['linear'].value of shape (18, 4096, 1024) (288.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['action_time_mlp_in']['kernel'].value of shape (2048, 1024) (8.00 MiB) along axis 0 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].mu['action_time_mlp_out']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value of shape (27, 1152, 4304) (510.68 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value of shape (27, 4304, 1152) (510.68 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value of shape (27, 16, 72, 1152) (136.69 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.612 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['head']['kernel'].value of shape (1152, 2048) (9.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['embedder']['input_embedding'].value of shape (257152, 2048) (2009.00 MiB) along axis 0 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value of shape (18, 8, 256, 2048) (288.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value of shape (18, 8, 256, 1024) (144.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value of shape (18, 2, 1, 2048, 256) (72.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value of shape (18, 2, 1, 1024, 256) (36.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value of shape (18, 8, 2048, 256) (288.00 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value of shape (18, 8, 1024, 256) (144.00 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value of shape (18, 2, 2048, 16384) (4608.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['mlp']['linear'].value of shape (18, 16384, 2048) (2304.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value of shape (18, 2, 1024, 4096) (576.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['mlp_1']['linear'].value of shape (18, 4096, 1024) (288.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['action_time_mlp_in']['kernel'].value of shape (2048, 1024) (8.00 MiB) along axis 0 (1523236:sharding.py:89)
14:15:54.613 [I] Sharding .opt_state[1][0].nu['action_time_mlp_out']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value of shape (27, 1152, 4304) (510.68 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value of shape (27, 4304, 1152) (510.68 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value of shape (27, 16, 72, 1152) (136.69 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['img']['head']['kernel'].value of shape (1152, 2048) (9.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['llm']['embedder']['input_embedding'].value of shape (257152, 2048) (2009.00 MiB) along axis 0 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value of shape (18, 8, 256, 2048) (288.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value of shape (18, 8, 256, 1024) (144.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value of shape (18, 2, 1, 2048, 256) (72.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value of shape (18, 2, 1, 1024, 256) (36.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value of shape (18, 8, 2048, 256) (288.00 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.615 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value of shape (18, 8, 1024, 256) (144.00 MiB) along axis 2 (1523236:sharding.py:89)
14:15:54.615 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value of shape (18, 2, 2048, 16384) (4608.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.615 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['mlp']['linear'].value of shape (18, 16384, 2048) (2304.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.615 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value of shape (18, 2, 1024, 4096) (576.00 MiB) along axis 3 (1523236:sharding.py:89)
14:15:54.615 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['mlp_1']['linear'].value of shape (18, 4096, 1024) (288.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.615 [I] Sharding .ema_params['action_time_mlp_in']['kernel'].value of shape (2048, 1024) (8.00 MiB) along axis 0 (1523236:sharding.py:89)
14:15:54.615 [I] Sharding .ema_params['action_time_mlp_out']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (1523236:sharding.py:89)
14:15:54.624 [I] Created BasePyTreeCheckpointHandler: use_ocdbt=True, use_zarr3=False, pytree_metadata_options=PyTreeMetadataOptions(support_rich_types=False), array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x1554b5977550> (1523236:base_pytree_checkpoint_handler.py:334)
14:15:54.964 [I] Restoring checkpoint from /home/congcong/.cache/openpi/openpi-assets/checkpoints/pi0_base/params. (1523236:checkpointer.py:298)
14:16:02.733 [I] [process=0] /jax/checkpoint/read/bytes_per_sec: 1.6 GiB/s (total bytes: 12.1 GiB) (time elapsed: 7 seconds) (per-host) (1523236:base_pytree_checkpoint_handler.py:114)
14:16:02.734 [I] Finished restoring checkpoint in 7.77 seconds from /home/congcong/.cache/openpi/openpi-assets/checkpoints/pi0_base/params. (1523236:checkpointer.py:309)
/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/jax/_src/interpreters/mlir.py:1178: UserWarning: Some donated buffers were not usable: ShapedArray(float32[27,1152,4304]), ShapedArray(float32[27,4304,1152]), ShapedArray(float32[27,1152,16,72]), ShapedArray(float32[27,16,72,1152]), ShapedArray(float32[27,1152,16,72]), ShapedArray(float32[27,1152,16,72]), ShapedArray(float32[1152,2048]), ShapedArray(float32[257152,2048]), ShapedArray(float32[18,8,256,2048]), ShapedArray(float32[18,8,256,1024]), ShapedArray(float32[18,2,1,2048,256]), ShapedArray(float32[18,2,1,1024,256]), ShapedArray(float32[18,8,2048,256]), ShapedArray(float32[18,8,1024,256]), ShapedArray(float32[18,2,2048,16384]), ShapedArray(float32[18,16384,2048]), ShapedArray(float32[18,2,1024,4096]), ShapedArray(float32[18,4096,1024]), ShapedArray(float32[2048,1024]), ShapedArray(float32[1024,1024]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn("Some donated buffers were not usable:"
14:16:11.254 [I] Initialized train state:
['PaliGemma']['img']['Transformer']['encoder_norm']['bias'].value: (1152,)@float32
['PaliGemma']['img']['Transformer']['encoder_norm']['scale'].value: (1152,)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['scale'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['scale'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['bias'].value: (27, 4304)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value: (27, 1152, 4304)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value: (27, 4304, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value: (27, 16, 72, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['embedding']['bias'].value: (1152,)@float32
['PaliGemma']['img']['embedding']['kernel'].value: (14, 14, 3, 1152)@float32
['PaliGemma']['img']['head']['bias'].value: (2048,)@float32
['PaliGemma']['img']['head']['kernel'].value: (1152, 2048)@float32
['PaliGemma']['img']['pos_embedding'].value: (1, 256, 1152)@float32
['PaliGemma']['llm']['embedder']['input_embedding'].value: (257152, 2048)@float32
['PaliGemma']['llm']['final_norm']['scale'].value: (2048,)@float32
['PaliGemma']['llm']['final_norm_1']['scale'].value: (1024,)@float32
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value: (18, 8, 256, 2048)@float32
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value: (18, 8, 256, 1024)@float32
['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value: (18, 2, 1, 2048, 256)@float32
['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value: (18, 2, 1, 1024, 256)@float32
['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value: (18, 8, 2048, 256)@float32
['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value: (18, 8, 1024, 256)@float32
['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value: (18, 2, 2048, 16384)@float32
['PaliGemma']['llm']['layers']['mlp']['linear'].value: (18, 16384, 2048)@float32
['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value: (18, 2, 1024, 4096)@float32
['PaliGemma']['llm']['layers']['mlp_1']['linear'].value: (18, 4096, 1024)@float32
['PaliGemma']['llm']['layers']['pre_attention_norm']['scale'].value: (18, 2048)@float32
['PaliGemma']['llm']['layers']['pre_attention_norm_1']['scale'].value: (18, 1024)@float32
['PaliGemma']['llm']['layers']['pre_ffw_norm']['scale'].value: (18, 2048)@float32
['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['scale'].value: (18, 1024)@float32
['action_in_proj']['bias'].value: (1024,)@float32
['action_in_proj']['kernel'].value: (32, 1024)@float32
['action_out_proj']['bias'].value: (32,)@float32
['action_out_proj']['kernel'].value: (1024, 32)@float32
['action_time_mlp_in']['bias'].value: (1024,)@float32
['action_time_mlp_in']['kernel'].value: (2048, 1024)@float32
['action_time_mlp_out']['bias'].value: (1024,)@float32
['action_time_mlp_out']['kernel'].value: (1024, 1024)@float32
['state_proj']['bias'].value: (1024,)@float32
['state_proj']['kernel'].value: (32, 1024)@float32 (1523236:train.py:238)
14:16:11.262 [I] Progress on: -/100000 rate:- remaining:? elapsed:00:00 postfix:-                 (1523236:tqdm_logging.py:145)
2026-01-04 14:16:15.336941: E external/xla/xla/service/slow_operation_alarm.cc:73] Constant folding an instruction is taking > 1s:

  %reduce-window.15 = s32[128,867]{1,0} reduce-window(%broadcast.5892, %constant.3394), window={size=1x867 pad=0_0x866_0}, to_apply=%region_45.13236, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(jit(cumsum))/reduce_window_sum" source_file="/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models/pi0.py" source_line=41}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
2026-01-04 14:16:19.441323: E external/xla/xla/service/slow_operation_alarm.cc:140] The operation took 5.104476149s
Constant folding an instruction is taking > 1s:

  %reduce-window.15 = s32[128,867]{1,0} reduce-window(%broadcast.5892, %constant.3394), window={size=1x867 pad=0_0x866_0}, to_apply=%region_45.13236, metadata={op_name="jit(<unnamed wrapped function>)/jit(main)/jvp(jit(cumsum))/reduce_window_sum" source_file="/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models/pi0.py" source_line=41}

This isn't necessarily a bug; constant-folding is inherently a trade-off between compilation time and speed at runtime. XLA has some guards that attempt to keep constant folding from taking too long, but fundamentally you'll always be able to come up with an input program that takes a long time.

If you'd like to file a bug, run with envvar XLA_FLAGS=--xla_dump_to=/tmp/foo and attach the results.
Step 0: grad_norm=2.0325, loss=0.2157, param_norm=1377.8652
14:16:54.132 [I] Progress on: -/100000 rate:- remaining:? elapsed:00:42 postfix:-                 (1523236:tqdm_logging.py:145)
14:17:04.430 [I] Progress on: 11.0it/100kit rate:1.6s/it remaining:43:26:31 elapsed:00:53 postfix:- (1523236:tqdm_logging.py:145)
14:17:15.551 [I] Progress on: 21.0it/100kit rate:1.2s/it remaining:33:42:13 elapsed:01:04 postfix:- (1523236:tqdm_logging.py:145)
14:17:26.502 [I] Progress on: 31.0it/100kit rate:1.2s/it remaining:33:51:39 elapsed:01:15 postfix:- (1523236:tqdm_logging.py:145)
14:17:36.691 [I] Progress on: 41.0it/100kit rate:1.0it/s remaining:27:18:23 elapsed:01:25 postfix:- (1523236:tqdm_logging.py:145)
14:17:46.989 [I] Progress on: 51.0it/100kit rate:1.0s/it remaining:27:55:05 elapsed:01:35 postfix:- (1523236:tqdm_logging.py:145)
14:17:57.195 [I] Progress on: 61.0it/100kit rate:1.0it/s remaining:27:10:17 elapsed:01:45 postfix:- (1523236:tqdm_logging.py:145)
14:18:07.496 [I] Progress on: 71.0it/100kit rate:1.1it/s remaining:26:21:51 elapsed:01:56 postfix:- (1523236:tqdm_logging.py:145)
14:18:18.733 [I] Progress on: 82.0it/100kit rate:1.0s/it remaining:28:55:53 elapsed:02:07 postfix:- (1523236:tqdm_logging.py:145)
14:18:28.813 [I] Progress on: 92.0it/100kit rate:1.0s/it remaining:28:07:19 elapsed:02:17 postfix:- (1523236:tqdm_logging.py:145)
Step 100: grad_norm=0.8318, loss=0.1681, param_norm=1377.8651
14:18:39.400 [I] Progress on: 103it/100kit rate:1.1it/s remaining:24:29:47 elapsed:02:28 postfix:- (1523236:tqdm_logging.py:145)
14:18:49.419 [I] Progress on: 113it/100kit rate:1.1it/s remaining:25:31:58 elapsed:02:38 postfix:- (1523236:tqdm_logging.py:145)
14:19:00.817 [I] Progress on: 124it/100kit rate:1.1s/it remaining:30:10:22 elapsed:02:49 postfix:- (1523236:tqdm_logging.py:145)
14:19:11.296 [I] Progress on: 135it/100kit rate:1.1it/s remaining:25:44:31 elapsed:03:00 postfix:- (1523236:tqdm_logging.py:145)
14:19:22.203 [I] Progress on: 146it/100kit rate:1.0s/it remaining:27:49:29 elapsed:03:10 postfix:- (1523236:tqdm_logging.py:145)
14:19:32.286 [I] Progress on: 157it/100kit rate:1.1it/s remaining:24:35:55 elapsed:03:21 postfix:- (1523236:tqdm_logging.py:145)
14:19:43.505 [I] Progress on: 168it/100kit rate:1.0s/it remaining:29:01:19 elapsed:03:32 postfix:- (1523236:tqdm_logging.py:145)
14:19:54.076 [I] Progress on: 179it/100kit rate:1.1it/s remaining:25:37:56 elapsed:03:42 postfix:- (1523236:tqdm_logging.py:145)
14:20:04.413 [I] Progress on: 189it/100kit rate:1.1it/s remaining:26:00:44 elapsed:03:53 postfix:- (1523236:tqdm_logging.py:145)
14:20:14.834 [I] Progress on: 200it/100kit rate:1.0it/s remaining:27:06:27 elapsed:04:03 postfix:- (1523236:tqdm_logging.py:145)
Step 200: grad_norm=0.5249, loss=0.1311, param_norm=1377.8656
14:20:25.315 [I] Progress on: 211it/100kit rate:1.1it/s remaining:26:09:40 elapsed:04:14 postfix:- (1523236:tqdm_logging.py:145)
14:20:35.605 [I] Progress on: 222it/100kit rate:1.0it/s remaining:26:31:52 elapsed:04:24 postfix:- (1523236:tqdm_logging.py:145)
14:20:45.767 [I] Progress on: 233it/100kit rate:1.1it/s remaining:24:47:54 elapsed:04:34 postfix:- (1523236:tqdm_logging.py:145)
14:20:56.318 [I] Progress on: 244it/100kit rate:1.0it/s remaining:26:59:51 elapsed:04:45 postfix:- (1523236:tqdm_logging.py:145)
14:21:06.537 [I] Progress on: 255it/100kit rate:1.1it/s remaining:24:51:52 elapsed:04:55 postfix:- (1523236:tqdm_logging.py:145)
14:21:18.090 [I] Progress on: 266it/100kit rate:1.1s/it remaining:31:04:54 elapsed:05:06 postfix:- (1523236:tqdm_logging.py:145)
14:21:28.440 [I] Progress on: 277it/100kit rate:1.1it/s remaining:25:46:25 elapsed:05:17 postfix:- (1523236:tqdm_logging.py:145)
14:21:39.072 [I] Progress on: 288it/100kit rate:1.0it/s remaining:27:28:34 elapsed:05:27 postfix:- (1523236:tqdm_logging.py:145)
14:21:49.218 [I] Progress on: 299it/100kit rate:1.1it/s remaining:25:30:08 elapsed:05:37 postfix:- (1523236:tqdm_logging.py:145)
Step 300: grad_norm=0.4697, loss=0.1025, param_norm=1377.8678
14:21:59.470 [I] Progress on: 310it/100kit rate:1.0it/s remaining:26:42:28 elapsed:05:48 postfix:- (1523236:tqdm_logging.py:145)
14:22:10.084 [I] Progress on: 321it/100kit rate:1.1it/s remaining:25:28:03 elapsed:05:58 postfix:- (1523236:tqdm_logging.py:145)
14:22:20.942 [I] Progress on: 332it/100kit rate:1.0it/s remaining:27:33:49 elapsed:06:09 postfix:- (1523236:tqdm_logging.py:145)
14:22:31.044 [I] Progress on: 343it/100kit rate:1.1it/s remaining:24:18:38 elapsed:06:19 postfix:- (1523236:tqdm_logging.py:145)
14:22:41.735 [I] Progress on: 354it/100kit rate:1.0it/s remaining:26:23:28 elapsed:06:30 postfix:- (1523236:tqdm_logging.py:145)
14:22:51.787 [I] Progress on: 365it/100kit rate:1.2it/s remaining:23:33:43 elapsed:06:40 postfix:- (1523236:tqdm_logging.py:145)
14:23:01.954 [I] Progress on: 376it/100kit rate:1.0it/s remaining:27:06:13 elapsed:06:50 postfix:- (1523236:tqdm_logging.py:145)
14:23:12.753 [I] Progress on: 388it/100kit rate:1.1it/s remaining:26:18:12 elapsed:07:01 postfix:- (1523236:tqdm_logging.py:145)
14:23:23.904 [I] Progress on: 400it/100kit rate:1.0s/it remaining:27:54:48 elapsed:07:12 postfix:- (1523236:tqdm_logging.py:145)
Step 400: grad_norm=0.4311, loss=0.0973, param_norm=1377.8722
14:23:34.875 [I] Progress on: 412it/100kit rate:1.1it/s remaining:25:21:02 elapsed:07:23 postfix:- (1523236:tqdm_logging.py:145)
14:23:45.691 [I] Progress on: 424it/100kit rate:1.1it/s remaining:25:37:16 elapsed:07:34 postfix:- (1523236:tqdm_logging.py:145)
14:23:56.503 [I] Progress on: 436it/100kit rate:1.1it/s remaining:25:42:53 elapsed:07:45 postfix:- (1523236:tqdm_logging.py:145)
14:24:07.804 [I] Progress on: 448it/100kit rate:1.0s/it remaining:28:15:30 elapsed:07:56 postfix:- (1523236:tqdm_logging.py:145)
14:24:18.094 [I] Progress on: 460it/100kit rate:1.1it/s remaining:24:33:15 elapsed:08:06 postfix:- (1523236:tqdm_logging.py:145)
14:24:28.904 [I] Progress on: 472it/100kit rate:1.1it/s remaining:26:00:37 elapsed:08:17 postfix:- (1523236:tqdm_logging.py:145)
14:24:39.682 [I] Progress on: 484it/100kit rate:1.1it/s remaining:25:56:27 elapsed:08:28 postfix:- (1523236:tqdm_logging.py:145)
14:24:50.059 [I] Progress on: 496it/100kit rate:1.2it/s remaining:24:01:24 elapsed:08:38 postfix:- (1523236:tqdm_logging.py:145)
Step 500: grad_norm=0.4019, loss=0.0864, param_norm=1377.8801
14:25:00.725 [I] Progress on: 508it/100kit rate:1.1it/s remaining:24:48:27 elapsed:08:49 postfix:- (1523236:tqdm_logging.py:145)
14:25:11.245 [I] Progress on: 520it/100kit rate:1.0it/s remaining:26:39:49 elapsed:08:59 postfix:- (1523236:tqdm_logging.py:145)
14:25:22.095 [I] Progress on: 532it/100kit rate:1.0s/it remaining:28:04:22 elapsed:09:10 postfix:- (1523236:tqdm_logging.py:145)
14:25:32.317 [I] Progress on: 543it/100kit rate:1.1it/s remaining:25:02:50 elapsed:09:21 postfix:- (1523236:tqdm_logging.py:145)
14:25:42.658 [I] Progress on: 555it/100kit rate:1.2it/s remaining:22:24:05 elapsed:09:31 postfix:- (1523236:tqdm_logging.py:145)
14:25:52.811 [I] Progress on: 567it/100kit rate:1.2it/s remaining:22:28:57 elapsed:09:41 postfix:- (1523236:tqdm_logging.py:145)
14:26:03.375 [I] Progress on: 579it/100kit rate:1.2it/s remaining:22:34:34 elapsed:09:52 postfix:- (1523236:tqdm_logging.py:145)
14:26:13.896 [I] Progress on: 591it/100kit rate:1.2it/s remaining:22:17:11 elapsed:10:02 postfix:- (1523236:tqdm_logging.py:145)
Step 600: grad_norm=0.4164, loss=0.0934, param_norm=1377.8917
14:26:24.512 [I] Progress on: 603it/100kit rate:1.1it/s remaining:24:00:53 elapsed:10:13 postfix:- (1523236:tqdm_logging.py:145)
14:26:34.822 [I] Progress on: 615it/100kit rate:1.2it/s remaining:22:06:47 elapsed:10:23 postfix:- (1523236:tqdm_logging.py:145)
14:26:45.052 [I] Progress on: 626it/100kit rate:1.0it/s remaining:26:37:35 elapsed:10:33 postfix:- (1523236:tqdm_logging.py:145)
14:26:55.397 [I] Progress on: 638it/100kit rate:1.2it/s remaining:23:38:26 elapsed:10:44 postfix:- (1523236:tqdm_logging.py:145)
14:27:05.853 [I] Progress on: 650it/100kit rate:1.1it/s remaining:24:44:46 elapsed:10:54 postfix:- (1523236:tqdm_logging.py:145)
14:27:15.932 [I] Progress on: 662it/100kit rate:1.2it/s remaining:23:19:39 elapsed:11:04 postfix:- (1523236:tqdm_logging.py:145)
14:27:26.486 [I] Progress on: 674it/100kit rate:1.1it/s remaining:25:56:01 elapsed:11:15 postfix:- (1523236:tqdm_logging.py:145)
14:27:37.059 [I] Progress on: 686it/100kit rate:1.1it/s remaining:24:33:05 elapsed:11:25 postfix:- (1523236:tqdm_logging.py:145)
14:27:47.583 [I] Progress on: 698it/100kit rate:1.1it/s remaining:24:06:55 elapsed:11:36 postfix:- (1523236:tqdm_logging.py:145)
Step 700: grad_norm=0.3697, loss=0.0975, param_norm=1377.9087
14:27:57.980 [I] Progress on: 710it/100kit rate:1.1it/s remaining:25:04:25 elapsed:11:46 postfix:- (1523236:tqdm_logging.py:145)
14:28:08.415 [I] Progress on: 722it/100kit rate:1.1it/s remaining:24:03:28 elapsed:11:57 postfix:- (1523236:tqdm_logging.py:145)
14:28:18.572 [I] Progress on: 734it/100kit rate:1.2it/s remaining:23:10:11 elapsed:12:07 postfix:- (1523236:tqdm_logging.py:145)
14:28:29.112 [I] Progress on: 746it/100kit rate:1.0it/s remaining:27:27:15 elapsed:12:17 postfix:- (1523236:tqdm_logging.py:145)
14:28:39.275 [I] Progress on: 758it/100kit rate:1.1it/s remaining:25:09:56 elapsed:12:28 postfix:- (1523236:tqdm_logging.py:145)
14:28:49.813 [I] Progress on: 771it/100kit rate:1.3it/s remaining:21:49:02 elapsed:12:38 postfix:- (1523236:tqdm_logging.py:145)
14:29:00.040 [I] Progress on: 783it/100kit rate:1.3it/s remaining:21:45:59 elapsed:12:48 postfix:- (1523236:tqdm_logging.py:145)
14:29:10.279 [I] Progress on: 795it/100kit rate:1.2it/s remaining:22:32:34 elapsed:12:59 postfix:- (1523236:tqdm_logging.py:145)
Step 800: grad_norm=0.3757, loss=0.0898, param_norm=1377.9320
14:29:20.443 [I] Progress on: 807it/100kit rate:1.2it/s remaining:23:04:57 elapsed:13:09 postfix:- (1523236:tqdm_logging.py:145)
14:29:30.465 [I] Progress on: 818it/100kit rate:1.1it/s remaining:25:35:42 elapsed:13:19 postfix:- (1523236:tqdm_logging.py:145)
14:29:40.614 [I] Progress on: 830it/100kit rate:1.1it/s remaining:24:25:02 elapsed:13:29 postfix:- (1523236:tqdm_logging.py:145)
14:29:50.630 [I] Progress on: 842it/100kit rate:1.1it/s remaining:25:01:31 elapsed:13:39 postfix:- (1523236:tqdm_logging.py:145)
14:30:01.291 [I] Progress on: 855it/100kit rate:1.2it/s remaining:22:13:16 elapsed:13:50 postfix:- (1523236:tqdm_logging.py:145)
14:30:11.520 [I] Progress on: 867it/100kit rate:1.3it/s remaining:21:54:39 elapsed:14:00 postfix:- (1523236:tqdm_logging.py:145)
14:30:22.681 [I] Progress on: 880it/100kit rate:1.1it/s remaining:25:27:49 elapsed:14:11 postfix:- (1523236:tqdm_logging.py:145)
14:30:33.352 [I] Progress on: 892it/100kit rate:1.2it/s remaining:23:22:45 elapsed:14:22 postfix:- (1523236:tqdm_logging.py:145)
Step 900: grad_norm=0.3942, loss=0.0813, param_norm=1377.9633
14:30:43.599 [I] Progress on: 904it/100kit rate:1.2it/s remaining:23:40:41 elapsed:14:32 postfix:- (1523236:tqdm_logging.py:145)
14:30:54.138 [I] Progress on: 917it/100kit rate:1.2it/s remaining:22:26:37 elapsed:14:42 postfix:- (1523236:tqdm_logging.py:145)
14:31:04.367 [I] Progress on: 929it/100kit rate:1.3it/s remaining:21:43:06 elapsed:14:53 postfix:- (1523236:tqdm_logging.py:145)
14:31:14.420 [I] Progress on: 940it/100kit rate:1.1it/s remaining:25:54:45 elapsed:15:03 postfix:- (1523236:tqdm_logging.py:145)
14:31:24.978 [I] Progress on: 953it/100kit rate:1.2it/s remaining:22:11:53 elapsed:15:13 postfix:- (1523236:tqdm_logging.py:145)
14:31:35.562 [I] Progress on: 966it/100kit rate:1.2it/s remaining:22:41:19 elapsed:15:24 postfix:- (1523236:tqdm_logging.py:145)
14:31:45.766 [I] Progress on: 978it/100kit rate:1.2it/s remaining:23:22:30 elapsed:15:34 postfix:- (1523236:tqdm_logging.py:145)
14:31:56.189 [I] Progress on: 991it/100kit rate:1.3it/s remaining:21:42:03 elapsed:15:44 postfix:- (1523236:tqdm_logging.py:145)
Step 1000: grad_norm=0.3577, loss=0.0878, param_norm=1378.0031
14:32:04.512 [I] [process=0][thread=MainThread][wait_until_finished] No Save Finalize thread to wait for. Returning. (1523236:checkpoint_manager.py:1987)
14:32:04.512 [I] [process=0] Saving checkpoint at step 1000                                       (1523236:checkpoint_manager.py:1408)
14:32:04.513 [I] [process=0] Started async saving checkpoint to /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000. (1523236:async_checkpointer.py:439)
14:32:04.515 [I] Using ThreadSafeKeyValueSignalingClient                                          (1523236:signaling_client.py:332)
14:32:04.527 [I] Creating tmp directory /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000.orbax-checkpoint-tmp-0 (1523236:atomicity.py:144)
14:32:04.533 [I] Creating tmp directory /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000.orbax-checkpoint-tmp-0/params.orbax-checkpoint-tmp-2 (1523236:atomicity.py:144)
14:32:04.533 [I] Creating tmp directory /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000.orbax-checkpoint-tmp-0/assets.orbax-checkpoint-tmp-1 (1523236:atomicity.py:144)
14:32:04.533 [I] Creating tmp directory /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000.orbax-checkpoint-tmp-0/train_state.orbax-checkpoint-tmp-3 (1523236:atomicity.py:144)
14:32:04.534 [I] Wrote Metadata={'item_handlers': None, 'metrics': {}, 'performance_metrics': {}, 'init_timestamp_nsecs': 1767508324529373322, 'commit_timestamp_nsecs': None, 'custom_metadata': {}}, json={"item_handlers": null, "metrics": {}, "performance_metrics": {}, "init_timestamp_nsecs": 1767508324529373322, "commit_timestamp_nsecs": null, "custom_metadata": {}} to /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000.orbax-checkpoint-tmp-0/_CHECKPOINT_METADATA (1523236:checkpoint.py:186)
14:32:04.623 [I] Transferring arrays to host memory with options: use_replica_parallel=True, enable_pinned_host_transfer=False (1523236:replica_slices.py:341)
14:32:12.268 [I] Transferring arrays to host memory with options: use_replica_parallel=True, enable_pinned_host_transfer=False (1523236:replica_slices.py:341)
14:32:12.366 [I] [process=0][thread=array_type_handler] Wrote 50 array_metadata.ArrayMetadata to /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000.orbax-checkpoint-tmp-0/params.orbax-checkpoint-tmp-2/array_metadatas/process_0 (1523236:array_metadata_store.py:198)
14:32:18.665 [I] [process=0] /jax/checkpoint/write/blocking_bytes_per_sec: 873.0 MiB/s (total bytes: 12.1 GiB) (time elapsed: 14 seconds) (per-host) (1523236:base_pytree_checkpoint_handler.py:114)
14:32:18.693 [I] [process=0] /jax/checkpoint/write/blocking_bytes_per_sec: 2.6 GiB/s (total bytes: 36.2 GiB) (time elapsed: 14 seconds) (per-host) (1523236:base_pytree_checkpoint_handler.py:114)
14:32:18.700 [I] [process=0][thread=async_save] Background save thread started.                   (1523236:async_checkpointer.py:77)
14:32:18.700 [I] Finished blocking save in 14.19 seconds. Continuing to save asynchronously to /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000. (1523236:async_checkpointer.py:548)
14:32:18.702 [I] [process=0][thread=MainThread][step=1000] Starting CheckpointManager Save Finalize thread=save_finalize (1523236:checkpoint_manager.py:1453)
14:32:18.702 [I] [process=0][thread=save_finalize] Waiting for background save thread=async_save. (1523236:async_checkpointer.py:258)
14:32:18.703 [I] {'step': 1000, 'event_type': 'save', 'directory': '/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes', 'reached_preemption': False, 'preemption_received_at': None, 'synchronous': False, 'wait_for_prev_start_time': 1767508324.5123746, 'wait_for_prev_duration_secs': 0.00023031234741210938, 'checkpointer_blocking_start_time': 1767508324.512959, 'checkpointer_blocking_duration_secs': 14.189094543457031, 'get_old_steps_start_time': 1767508338.7020838, 'get_old_steps_duration_secs': 6.9141387939453125e-06, 'checkpoint_manager_blocking_start_time': 1767508324.5122201, 'checkpoint_manager_blocking_duration_secs': 14.190910816192627} (1523236:standard_logger.py:34)
14:32:18.704 [I] Progress on: 1.00kit/100kit rate:5.1s/it remaining:139:20:29 elapsed:16:07 postfix:- (1523236:tqdm_logging.py:145)
14:32:18.782 [I] [process=0][thread=array_type_handler] Wrote 153 array_metadata.ArrayMetadata to /project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/checkpoints/pi0_base_aloha_robotwin_full/folding_clothes/1000.orbax-checkpoint-tmp-0/train_state.orbax-checkpoint-tmp-3/array_metadatas/process_0 (1523236:array_metadata_store.py:198)
14:32:28.866 [I] Progress on: 1.00kit/100kit rate:6.6s/it remaining:181:22:20 elapsed:16:17 postfix:- (1523236:tqdm_logging.py:145)
