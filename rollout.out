Sat Dec 20 18:48:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   23C    P0             65W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   23C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   23C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   23C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[33mgpu id (to use): 0[0m
[32mRender Well[0m
{'policy_name': 'yzy_openpi', 'task_name': 'beat_block_hammer', 'task_config': 'demo_clean', 'ckpt_setting': 'beat_block_hammer', 'seed': 0, 'instruction_type': 'unseen', 'train_config_name': 'pi0_torch_from_jax', 'model_name': 'beat_block_hammer', 'checkpoint_id': 30000, 'pi0_step': 8, 'device': 'cuda:0'}
###########################Start Evaluation###########################
start process in gpu:  0
start process in gpu:  1
start process in gpu:  2
start process in gpu:  3
Task 0 (GPU 0) output:
Process 2489929: CUDA_VISIBLE_DEVICES = 0
Process 2489929: CUDA available: True
Process 2489929: Device count: 1
Process 2489929: Current device: 0
Process 2489929: Device name: NVIDIA H800
Process 2489929 LOCAL_RANK None

Task 0 errors:
W1220 19:45:07.894000 2489929 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 19:45:07.894000 2489929 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 19:45:07.894000 2489929 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 19:45:07.894000 2489929 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 19:45:07.894000 2489929 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[2025-12-20 21:01:09.644] [svulkan2] [error] OIDN Error: out of memory
[2025-12-20 21:01:09.786] [svulkan2] [error] OIDN Error: out of memory
Traceback (most recent call last):
  File "<string>", line 66, in <module>
  File "/project/peilab/yanzhengyang/RoboTwin/script/RobotwinEnvWrapper.py", line 196, in fm_evaluation
    data = eval_func(self.env, model, observation)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/./policy/yzy_openpi/deploy_policy_collect.py", line 51, in eval
    actions = model.get_action()[:model.pi0_step]
              ^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/pi_model.py", line 79, in get_action
    return self.policy.infer(self.observation_window)["actions"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/policies/policy.py", line 94, in infer
    "actions": self._sample_actions(sample_rng_or_pytorch_device, observation, **sample_kwargs),
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 383, in sample_actions
    images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=False)
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 383, in torch_dynamo_resume_in_sample_actions_at_383
    images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=False)
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 1209, in forward
    return compiled_fn(full_args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 328, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 495, in wrapper
    return compiled_fn(runtime_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1372, in run
    return compiled_fn(new_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 371, in deferred_cudagraphify
    return fn(inputs)
           ^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
           ^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1997, in run
    out = self._run(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 2175, in _run
    out = self.record_function(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 2211, in record_function
    node = CUDAGraphNode(
           ^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1021, in __init__
    self.recording_outputs: Optional[OutputType] = self._record(
                                                   ^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1248, in _record
    with (
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py", line 186, in __exit__
    self.cuda_graph.capture_end()
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py", line 84, in capture_end
    super().capture_end()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Task 1 (GPU 1) output:
Process 2489930: CUDA_VISIBLE_DEVICES = 1
Process 2489930: CUDA available: True
Process 2489930: Device count: 1
Process 2489930: Current device: 0
Process 2489930: Device name: NVIDIA H800
Process 2489930 LOCAL_RANK None

Task 1 errors:
W1220 19:46:35.154000 2489930 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 19:46:35.154000 2489930 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 19:46:35.154000 2489930 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 19:46:35.154000 2489930 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 19:46:35.154000 2489930 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[2025-12-20 20:59:53.502] [svulkan2] [error] OIDN Error: out of memory
Traceback (most recent call last):
  File "<string>", line 66, in <module>
  File "/project/peilab/yanzhengyang/RoboTwin/script/RobotwinEnvWrapper.py", line 196, in fm_evaluation
    data = eval_func(self.env, model, observation)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/./policy/yzy_openpi/deploy_policy_collect.py", line 51, in eval
    actions = model.get_action()[:model.pi0_step]
              ^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/pi_model.py", line 79, in get_action
    return self.policy.infer(self.observation_window)["actions"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/policies/policy.py", line 94, in infer
    "actions": self._sample_actions(sample_rng_or_pytorch_device, observation, **sample_kwargs),
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 383, in sample_actions
    images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=False)
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 406, in torch_dynamo_resume_in_sample_actions_at_383
    while time >= -dt / 2:
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 406, in torch_dynamo_resume_in_sample_actions_at_406
    while time >= -dt / 2:
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 1209, in forward
    return compiled_fn(full_args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 328, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 689, in inner_fn
    outs = compiled_fn(args)
           ^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 495, in wrapper
    return compiled_fn(runtime_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1372, in run
    return compiled_fn(new_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 371, in deferred_cudagraphify
    return fn(inputs)
           ^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
           ^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1997, in run
    out = self._run(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 2175, in _run
    out = self.record_function(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 2211, in record_function
    node = CUDAGraphNode(
           ^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1021, in __init__
    self.recording_outputs: Optional[OutputType] = self._record(
                                                   ^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1248, in _record
    with (
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py", line 186, in __exit__
    self.cuda_graph.capture_end()
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py", line 84, in capture_end
    super().capture_end()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Task 2 (GPU 2) output:
Process 2489931: CUDA_VISIBLE_DEVICES = 2
Process 2489931: CUDA available: True
Process 2489931: Device count: 1
Process 2489931: Current device: 0
Process 2489931: Device name: NVIDIA H800
Process 2489931 LOCAL_RANK None

Task 2 errors:
W1220 19:45:14.588000 2489931 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 19:45:14.588000 2489931 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 19:45:14.588000 2489931 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 19:45:14.588000 2489931 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 19:45:14.588000 2489931 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[2025-12-20 20:59:36.765] [svulkan2] [error] OIDN Error: out of memory
Traceback (most recent call last):
  File "<string>", line 66, in <module>
  File "/project/peilab/yanzhengyang/RoboTwin/script/RobotwinEnvWrapper.py", line 196, in fm_evaluation
    data = eval_func(self.env, model, observation)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/./policy/yzy_openpi/deploy_policy_collect.py", line 51, in eval
    actions = model.get_action()[:model.pi0_step]
              ^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/pi_model.py", line 79, in get_action
    return self.policy.infer(self.observation_window)["actions"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/policies/policy.py", line 94, in infer
    "actions": self._sample_actions(sample_rng_or_pytorch_device, observation, **sample_kwargs),
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 383, in sample_actions
    images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=False)
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 406, in torch_dynamo_resume_in_sample_actions_at_383
    while time >= -dt / 2:
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 406, in torch_dynamo_resume_in_sample_actions_at_406
    while time >= -dt / 2:
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 1209, in forward
    return compiled_fn(full_args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 328, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 689, in inner_fn
    outs = compiled_fn(args)
           ^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 495, in wrapper
    return compiled_fn(runtime_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1372, in run
    return compiled_fn(new_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 371, in deferred_cudagraphify
    return fn(inputs)
           ^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
           ^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1997, in run
    out = self._run(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 2175, in _run
    out = self.record_function(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 2211, in record_function
    node = CUDAGraphNode(
           ^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1021, in __init__
    self.recording_outputs: Optional[OutputType] = self._record(
                                                   ^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1248, in _record
    with (
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py", line 186, in __exit__
    self.cuda_graph.capture_end()
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py", line 84, in capture_end
    super().capture_end()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Task 3 (GPU 3) output:
Process 2489932: CUDA_VISIBLE_DEVICES = 3
Process 2489932: CUDA available: True
Process 2489932: Device count: 1
Process 2489932: Current device: 0
Process 2489932: Device name: NVIDIA H800
Process 2489932 LOCAL_RANK None

Task 3 errors:
W1220 19:43:53.164000 2489932 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 19:43:53.164000 2489932 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 19:43:53.164000 2489932 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 19:43:53.164000 2489932 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 19:43:53.164000 2489932 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
[2025-12-20 21:07:06.548] [svulkan2] [error] OIDN Error: out of memory
Traceback (most recent call last):
  File "<string>", line 66, in <module>
  File "/project/peilab/yanzhengyang/RoboTwin/script/RobotwinEnvWrapper.py", line 196, in fm_evaluation
    data = eval_func(self.env, model, observation)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/./policy/yzy_openpi/deploy_policy_collect.py", line 51, in eval
    actions = model.get_action()[:model.pi0_step]
              ^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/pi_model.py", line 79, in get_action
    return self.policy.infer(self.observation_window)["actions"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/policies/policy.py", line 94, in infer
    "actions": self._sample_actions(sample_rng_or_pytorch_device, observation, **sample_kwargs),
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py", line 70, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 383, in sample_actions
    images, img_masks, lang_tokens, lang_masks, state = self._preprocess_observation(observation, train=False)
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 406, in torch_dynamo_resume_in_sample_actions_at_383
    while time >= -dt / 2:
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/src/openpi/models_pytorch/pi0_pytorch.py", line 406, in torch_dynamo_resume_in_sample_actions_at_406
    while time >= -dt / 2:
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 838, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py", line 1209, in forward
    return compiled_fn(full_args)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 328, in runtime_wrapper
    all_outs = call_func_at_runtime_with_args(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py", line 126, in call_func_at_runtime_with_args
    out = normalize_as_list(f(args))
                            ^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 689, in inner_fn
    outs = compiled_fn(args)
           ^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 495, in wrapper
    return compiled_fn(runtime_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/output_code.py", line 460, in __call__
    return self.current_callable(inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/compile_fx.py", line 1372, in run
    return compiled_fn(new_inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 371, in deferred_cudagraphify
    return fn(inputs)
           ^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/utils.py", line 2404, in run
    return model(new_inputs)
           ^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1997, in run
    out = self._run(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 2175, in _run
    out = self.record_function(new_inputs, function_id)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 2211, in record_function
    node = CUDAGraphNode(
           ^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1021, in __init__
    self.recording_outputs: Optional[OutputType] = self._record(
                                                   ^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_inductor/cudagraph_trees.py", line 1248, in _record
    with (
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py", line 186, in __exit__
    self.cuda_graph.capture_end()
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/graphs.py", line 84, in capture_end
    super().capture_end()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


Process 2489503: Error in task 0: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 18:48:57_gpu_0_result.pkl'
Process 2489503: Error in task 1: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 18:48:57_gpu_1_result.pkl'
Process 2489503: Error in task 2: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 18:48:57_gpu_2_result.pkl'
Process 2489503: Error in task 3: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 18:48:57_gpu_3_result.pkl'
Total time: 8298.947407007217
Length of results: 0
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/script/distributed_loop_collect_fm.py", line 248, in <module>
    run_sapien_distributed_subprocess(usr_args, 4, 64, file_name = "beat_block_hammer.pkl")
  File "/project/peilab/yanzhengyang/RoboTwin/script/distributed_loop_collect_fm.py", line 226, in run_sapien_distributed_subprocess
    print(f"Success rate: {sum(all_results)/len(all_results)*100:.1f}%")
                           ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
ZeroDivisionError: division by zero
Sat Dec 20 21:19:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   23C    P0             65W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   23C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   23C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   23C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[33mgpu id (to use): 0[0m
[32mRender Well[0m
{'policy_name': 'yzy_openpi', 'task_name': 'beat_block_hammer', 'task_config': 'demo_clean', 'ckpt_setting': 'beat_block_hammer', 'seed': 0, 'instruction_type': 'unseen', 'train_config_name': 'pi0_torch_from_jax', 'model_name': 'beat_block_hammer', 'checkpoint_id': 30000, 'pi0_step': 8, 'device': 'cuda:0'}
###########################Start Evaluation###########################
start process in gpu:  0
start process in gpu:  1
start process in gpu:  2
start process in gpu:  3
start process in gpu:  4
start process in gpu:  5
start process in gpu:  6
start process in gpu:  7
Task 0 (GPU 0) output:
Process 3047393: CUDA_VISIBLE_DEVICES = 0
Process 3047393: CUDA available: True
Process 3047393: Device count: 1
Process 3047393: Current device: 0
Process 3047393: Device name: NVIDIA H800
Process 3047393 LOCAL_RANK None

Task 0 errors:
W1220 21:56:18.255000 3047393 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 21:56:18.255000 3047393 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 21:56:18.255000 3047393 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 21:56:18.255000 3047393 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 21:56:18.255000 3047393 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 1 (GPU 1) output:
Process 3047394: CUDA_VISIBLE_DEVICES = 1
Process 3047394: CUDA available: True
Process 3047394: Device count: 1
Process 3047394: Current device: 0
Process 3047394: Device name: NVIDIA H800
Process 3047394 LOCAL_RANK None

Task 1 errors:
W1220 21:56:28.993000 3047394 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 21:56:28.993000 3047394 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 21:56:28.993000 3047394 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 21:56:28.993000 3047394 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 21:56:28.993000 3047394 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 2 (GPU 2) output:
Process 3047395: CUDA_VISIBLE_DEVICES = 2
Process 3047395: CUDA available: True
Process 3047395: Device count: 1
Process 3047395: Current device: 0
Process 3047395: Device name: NVIDIA H800
Process 3047395 LOCAL_RANK None

Task 2 errors:
W1220 22:01:13.926000 3047395 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 22:01:13.926000 3047395 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 22:01:13.926000 3047395 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 22:01:13.926000 3047395 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 22:01:13.926000 3047395 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 3 (GPU 3) output:
Process 3047396: CUDA_VISIBLE_DEVICES = 3
Process 3047396: CUDA available: True
Process 3047396: Device count: 1
Process 3047396: Current device: 0
Process 3047396: Device name: NVIDIA H800
Process 3047396 LOCAL_RANK None

Task 3 errors:
W1220 21:58:59.661000 3047396 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 21:58:59.661000 3047396 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 21:58:59.661000 3047396 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 21:58:59.661000 3047396 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 21:58:59.661000 3047396 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 4 (GPU 4) output:
Process 3047397: CUDA_VISIBLE_DEVICES = 4
Process 3047397: CUDA available: False
Process 3047397: Device count: 0
Process 3047397 LOCAL_RANK None

Task 4 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 5 (GPU 5) output:
Process 3047398: CUDA_VISIBLE_DEVICES = 5
Process 3047398: CUDA available: False
Process 3047398: Device count: 0
Process 3047398 LOCAL_RANK None

Task 5 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 6 (GPU 6) output:
Process 3047399: CUDA_VISIBLE_DEVICES = 6
Process 3047399: CUDA available: False
Process 3047399: Device count: 0
Process 3047399 LOCAL_RANK None

Task 6 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 7 (GPU 7) output:
Process 3047400: CUDA_VISIBLE_DEVICES = 7
Process 3047400: CUDA available: False
Process 3047400: Device count: 0
Process 3047400 LOCAL_RANK None

Task 7 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Process 3046800: Error in task 4: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 21:20:18_gpu_4_result.pkl'
Process 3046800: Error in task 5: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 21:20:18_gpu_5_result.pkl'
Process 3046800: Error in task 6: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 21:20:18_gpu_6_result.pkl'
Process 3046800: Error in task 7: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 21:20:18_gpu_7_result.pkl'
Total time: 4374.760416030884
Length of results: 64
Success rate: 40.6%
Sat Dec 20 22:57:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   23C    P0             65W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   23C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   23C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   23C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[33mgpu id (to use): 0[0m
[32mRender Well[0m
{'policy_name': 'yzy_openpi', 'task_name': 'beat_block_hammer', 'task_config': 'demo_clean', 'ckpt_setting': 'beat_block_hammer', 'seed': 0, 'instruction_type': 'unseen', 'train_config_name': 'pi0_torch_from_jax', 'model_name': 'beat_block_hammer', 'checkpoint_id': 30000, 'pi0_step': 8, 'device': 'cuda:0'}
###########################Start Evaluation###########################
start process in gpu:  0
start process in gpu:  1
start process in gpu:  2
start process in gpu:  3
start process in gpu:  4
start process in gpu:  5
start process in gpu:  6
start process in gpu:  7
Task 0 (GPU 0) output:
Process 3297507: CUDA_VISIBLE_DEVICES = 0
Process 3297507: CUDA available: True
Process 3297507: Device count: 1
Process 3297507: Current device: 0
Process 3297507: Device name: NVIDIA H800
Process 3297507 LOCAL_RANK None

Task 0 errors:
W1220 23:39:05.349000 3297507 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 23:39:05.349000 3297507 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 23:39:05.349000 3297507 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 23:39:05.349000 3297507 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 23:39:05.349000 3297507 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 1 (GPU 1) output:
Process 3297508: CUDA_VISIBLE_DEVICES = 1
Process 3297508: CUDA available: True
Process 3297508: Device count: 1
Process 3297508: Current device: 0
Process 3297508: Device name: NVIDIA H800
Process 3297508 LOCAL_RANK None

Task 1 errors:
W1220 23:40:16.001000 3297508 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 23:40:16.001000 3297508 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 23:40:16.001000 3297508 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 23:40:16.001000 3297508 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 23:40:16.001000 3297508 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 2 (GPU 2) output:
Process 3297509: CUDA_VISIBLE_DEVICES = 2
Process 3297509: CUDA available: True
Process 3297509: Device count: 1
Process 3297509: Current device: 0
Process 3297509: Device name: NVIDIA H800
Process 3297509 LOCAL_RANK None

Task 2 errors:
W1220 23:34:53.601000 3297509 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 23:34:53.601000 3297509 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 23:34:53.601000 3297509 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 23:34:53.601000 3297509 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 23:34:53.601000 3297509 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 3 (GPU 3) output:
Process 3297510: CUDA_VISIBLE_DEVICES = 3
Process 3297510: CUDA available: True
Process 3297510: Device count: 1
Process 3297510: Current device: 0
Process 3297510: Device name: NVIDIA H800
Process 3297510 LOCAL_RANK None

Task 3 errors:
W1220 23:36:47.346000 3297510 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1220 23:36:47.346000 3297510 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1220 23:36:47.346000 3297510 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1220 23:36:47.346000 3297510 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1220 23:36:47.346000 3297510 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 4 (GPU 4) output:
Process 3297511: CUDA_VISIBLE_DEVICES = 4
Process 3297511: CUDA available: False
Process 3297511: Device count: 0
Process 3297511 LOCAL_RANK None

Task 4 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 5 (GPU 5) output:
Process 3297512: CUDA_VISIBLE_DEVICES = 5
Process 3297512: CUDA available: False
Process 3297512: Device count: 0
Process 3297512 LOCAL_RANK None

Task 5 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 6 (GPU 6) output:
Process 3297513: CUDA_VISIBLE_DEVICES = 6
Process 3297513: CUDA available: False
Process 3297513: Device count: 0
Process 3297513 LOCAL_RANK None

Task 6 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 7 (GPU 7) output:
Process 3297514: CUDA_VISIBLE_DEVICES = 7
Process 3297514: CUDA available: False
Process 3297514: Device count: 0
Process 3297514 LOCAL_RANK None

Task 7 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Process 3297048: Error in task 4: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 22:58:21_gpu_4_result.pkl'
Process 3297048: Error in task 5: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 22:58:21_gpu_5_result.pkl'
Process 3297048: Error in task 6: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 22:58:21_gpu_6_result.pkl'
Process 3297048: Error in task 7: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-20 22:58:21_gpu_7_result.pkl'
Total time: 4656.029727220535
Length of results: 64
Success rate: 31.2%
Sun Dec 21 01:01:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   25C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   23C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   26C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   26C    P0             67W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[33mgpu id (to use): 0[0m
[32mRender Well[0m
{'policy_name': 'yzy_openpi', 'task_name': 'beat_block_hammer', 'task_config': 'demo_clean', 'ckpt_setting': 'beat_block_hammer', 'seed': 0, 'instruction_type': 'unseen', 'train_config_name': 'pi0_torch_from_jax', 'model_name': 'beat_block_hammer', 'checkpoint_id': 30000, 'pi0_step': 8, 'device': 'cuda:0'}
###########################Start Evaluation###########################
start process in gpu:  0
start process in gpu:  1
start process in gpu:  2
start process in gpu:  3
start process in gpu:  4
start process in gpu:  5
start process in gpu:  6
start process in gpu:  7
slurmstepd: error: *** JOB 330967 ON dgx-46 CANCELLED AT 2025-12-21T01:31:22 DUE TO PREEMPTION ***
Sun Dec 21 01:51:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:1B:00.0 Off |                    0 |
| N/A   25C    P0             72W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:43:00.0 Off |                    0 |
| N/A   27C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:52:00.0 Off |                    0 |
| N/A   26C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   25C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[33mgpu id (to use): 0[0m
[32mRender Well[0m
{'policy_name': 'yzy_openpi', 'task_name': 'beat_block_hammer', 'task_config': 'demo_clean', 'ckpt_setting': 'beat_block_hammer', 'seed': 0, 'instruction_type': 'unseen', 'train_config_name': 'pi0_torch_from_jax', 'model_name': 'beat_block_hammer', 'checkpoint_id': 30000, 'pi0_step': 8, 'device': 'cuda:0'}
###########################Start Evaluation###########################
start process in gpu:  0
start process in gpu:  1
start process in gpu:  2
start process in gpu:  3
start process in gpu:  4
start process in gpu:  5
start process in gpu:  6
start process in gpu:  7
Sun Dec 21 02:46:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H800                    On  |   00000000:9D:00.0 Off |                    0 |
| N/A   22C    P0             70W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H800                    On  |   00000000:C3:00.0 Off |                    0 |
| N/A   23C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H800                    On  |   00000000:D1:00.0 Off |                    0 |
| N/A   24C    P0             66W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H800                    On  |   00000000:DF:00.0 Off |                    0 |
| N/A   24C    P0             68W /  700W |       0MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[33mgpu id (to use): 0[0m
[32mRender Well[0m
{'policy_name': 'yzy_openpi', 'task_name': 'beat_block_hammer', 'task_config': 'demo_clean', 'ckpt_setting': 'beat_block_hammer', 'seed': 0, 'instruction_type': 'unseen', 'train_config_name': 'pi0_torch_from_jax', 'model_name': 'beat_block_hammer', 'checkpoint_id': 30000, 'pi0_step': 8, 'device': 'cuda:0'}
###########################Start Evaluation###########################
start process in gpu:  0
start process in gpu:  1
start process in gpu:  2
start process in gpu:  3
start process in gpu:  4
start process in gpu:  5
start process in gpu:  6
start process in gpu:  7
Task 0 (GPU 0) output:
Process 192962: CUDA_VISIBLE_DEVICES = 0
Process 192962: CUDA available: True
Process 192962: Device count: 1
Process 192962: Current device: 0
Process 192962: Device name: NVIDIA H800
Process 192962 LOCAL_RANK None

Task 0 errors:
AUTOTUNE mm(816x2048, 2048x16384)
  mm 0.0780 ms 100.0% 
  triton_mm_9478 0.1038 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9479 0.1038 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9477 0.1293 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9473 0.1293 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9472 0.1295 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9470 0.1648 ms 47.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9471 0.1687 ms 46.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9468 0.1721 ms 45.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9474 0.1748 ms 44.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.1877 seconds and 67.4784 seconds precompiling for 20 choices
AUTOTUNE mm(256x1152, 1152x4304)
  mm 0.0122 ms 100.0% 
  triton_mm_101 0.0139 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_94 0.0153 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_90 0.0154 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_100 0.0159 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_95 0.0165 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_91 0.0179 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_97 0.0181 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_93 0.0182 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_92 0.0214 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2837 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(816x2048, 2048x2048)
  mm 0.0160 ms 100.0% 
  triton_mm_9441 0.0175 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9435 0.0220 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9434 0.0223 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9440 0.0231 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9430 0.0238 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9433 0.0258 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9437 0.0268 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9431 0.0287 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9436 0.0333 ms 48.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3118 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(256x1152, 1152x1152)
  mm 0.0090 ms 100.0% 
  triton_mm_72 0.0091 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_76 0.0101 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_68 0.0114 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_82 0.0124 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_71 0.0132 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_75 0.0135 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_65 0.0140 ms 64.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_67 0.0145 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_81 0.0147 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2760 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE convolution(1x3x224x224, 1152x3x14x14)
  convolution 0.0642 ms 100.0% 
  triton_convolution2d_4 0.1532 ms 41.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_3 0.1971 ms 32.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_5 0.2146 ms 29.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_6 0.2170 ms 29.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_2 0.3495 ms 18.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=1, num_warps=8
  triton_convolution2d_1 0.4086 ms 15.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_0 0.7454 ms 8.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5982 seconds and 0.0001 seconds precompiling for 8 choices
AUTOTUNE mm(256x4304, 4304x1152)
  mm 0.0150 ms 100.0% 
  triton_mm_110 0.0178 ms 84.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_114 0.0219 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_106 0.0253 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_120 0.0297 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_109 0.0355 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_113 0.0369 ms 40.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_105 0.0380 ms 39.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_103 0.0385 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_119 0.0395 ms 38.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3506 seconds and 0.0001 seconds precompiling for 20 choices
AUTOTUNE mm(256x4304, 4304x1152)
  mm 0.0147 ms 100.0% 
  triton_mm_3651 0.0181 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_3655 0.0223 ms 65.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_3647 0.0258 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_3661 0.0298 ms 49.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_3650 0.0356 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_3654 0.0365 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_3646 0.0382 ms 38.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_3644 0.0382 ms 38.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_3660 0.0392 ms 37.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3514 seconds and 0.0000 seconds precompiling for 20 choices
AUTOTUNE addmm(256x2048, 256x1152, 1152x2048)
  bias_addmm 0.0096 ms 100.0% 
  triton_mm_9263 0.0102 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9267 0.0114 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9259 0.0132 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9273 0.0135 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  addmm 0.0138 ms 70.0% 
  triton_mm_9262 0.0142 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9266 0.0146 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9258 0.0151 ms 63.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9256 0.0160 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2852 seconds and 0.0001 seconds precompiling for 21 choices
AUTOTUNE bmm(1x128x1, 1x1x816)
  triton_bmm_9332 0.0052 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_9334 0.0054 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9331 0.0055 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_bmm_9333 0.0055 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9338 0.0056 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_9335 0.0056 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_9336 0.0056 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_9337 0.0059 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9342 0.0064 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9340 0.0065 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2028 seconds and 0.0001 seconds precompiling for 17 choices
AUTOTUNE mm(816x2048, 2048x256)
  mm 0.0100 ms 100.0% 
  triton_mm_9351 0.0106 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9355 0.0110 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9359 0.0128 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9365 0.0164 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9350 0.0171 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9354 0.0177 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9348 0.0183 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_9349 0.0183 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9358 0.0187 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2786 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE bmm(8x816x256, 8x256x816)
  bmm 0.0138 ms 100.0% 
  triton_bmm_9382 0.0139 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9377 0.0153 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9380 0.0167 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9383 0.0168 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9376 0.0170 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9379 0.0170 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9384 0.0171 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9373 0.0184 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9375 0.0185 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2761 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE bmm(8x816x816, 8x816x256)
  bmm 0.0125 ms 100.0% 
  triton_bmm_9422 0.0140 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9415 0.0146 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9421 0.0150 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9411 0.0156 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9414 0.0161 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9418 0.0165 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9416 0.0173 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_9413 0.0182 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9417 0.0191 ms 65.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2698 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(816x16384, 16384x2048)
  mm 0.0875 ms 100.0% 
  triton_mm_9498 0.0991 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9491 0.1252 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9497 0.1312 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9492 0.1425 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9490 0.1597 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9488 0.1672 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9494 0.1703 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9487 0.1899 ms 46.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9489 0.2095 ms 41.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6407 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(51x1024, 1024x4096)
  mm 0.0100 ms 100.0% 
  triton_mm_12474 0.0101 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12478 0.0104 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12482 0.0113 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12486 0.0121 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12477 0.0131 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12473 0.0133 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12481 0.0137 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12471 0.0140 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12472 0.0140 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2558 seconds and 3.2028 seconds precompiling for 18 choices
E1221 02:07:21.091000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:21.091000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:21.091000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:21.154000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:21.154000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 327680, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:21.154000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:21.208000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:21.208000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 393216, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:21.208000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:21.264000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:21.264000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:21.264000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
AUTOTUNE mm(50x2048, 2048x1024)
  mm 0.0152 ms 100.0% 
  triton_mm_12305 0.0200 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12309 0.0225 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12313 0.0227 ms 66.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12304 0.0237 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12316 0.0242 ms 62.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12303 0.0255 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12312 0.0261 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12315 0.0308 ms 49.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12311 0.0326 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2600 seconds and 0.0002 seconds precompiling for 18 choices
E1221 02:07:55.642000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.642000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.642000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.696000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.696000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 327680, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.696000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.748000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.748000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 393216, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.748000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.799000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.799000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.799000 192962 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
AUTOTUNE addmm(50x1024, 50x1024, 1024x1024)
  bias_addmm 0.0120 ms 100.0% 
  triton_mm_12322 0.0129 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12321 0.0143 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12326 0.0149 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12320 0.0152 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12333 0.0155 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12330 0.0159 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  addmm 0.0166 ms 72.4% 
  triton_mm_12329 0.0169 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12332 0.0181 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2372 seconds and 0.0002 seconds precompiling for 19 choices
AUTOTUNE addmm(50x1024, 50x32, 32x1024)
  triton_mm_12287 0.0056 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12289 0.0061 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12293 0.0061 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12288 0.0061 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12299 0.0061 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12298 0.0063 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12292 0.0065 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12294 0.0065 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12290 0.0067 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12286 0.0069 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.2098 seconds and 0.0001 seconds precompiling for 18 choices
AUTOTUNE mm(51x1024, 1024x2048)
  mm 0.0087 ms 100.0% 
  triton_mm_12340 0.0092 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12344 0.0095 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12348 0.0103 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12352 0.0110 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12339 0.0123 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12343 0.0126 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12347 0.0129 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12337 0.0130 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12338 0.0130 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2362 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(1x128x1, 1x1x51)
  triton_bmm_12353 0.0053 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_bmm_12354 0.0053 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12357 0.0054 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12355 0.0054 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12359 0.0057 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12361 0.0057 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12363 0.0057 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12356 0.0057 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12362 0.0058 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_12358 0.0060 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.1875 seconds and 0.0001 seconds precompiling for 16 choices
AUTOTUNE mm(51x1024, 1024x256)
  mm 0.0073 ms 100.0% 
  triton_mm_12372 0.0077 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12376 0.0081 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12380 0.0092 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12384 0.0097 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12371 0.0107 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12375 0.0108 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12379 0.0112 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12369 0.0114 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12370 0.0114 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2273 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(8x51x256, 8x256x867)
  triton_bmm_12393 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12392 0.0078 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12399 0.0079 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_12401 0.0080 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12396 0.0083 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12397 0.0083 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12387 0.0085 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12388 0.0085 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12386 0.0086 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12395 0.0087 ms 88.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2186 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(8x56x872, 8x872x256)
  bmm 0.0087 ms 100.0% 
  triton_bmm_12423 0.0094 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12427 0.0097 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12431 0.0104 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12435 0.0114 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12422 0.0115 ms 75.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12421 0.0117 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12426 0.0119 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12430 0.0126 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12433 0.0131 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2326 seconds and 0.0001 seconds precompiling for 18 choices
AUTOTUNE mm(51x4096, 4096x1024)
  mm 0.0142 ms 100.0% 
  triton_mm_12491 0.0143 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12495 0.0152 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12499 0.0221 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12503 0.0243 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12490 0.0305 ms 46.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12489 0.0319 ms 44.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12498 0.0319 ms 44.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12494 0.0320 ms 44.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12488 0.0346 ms 41.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2971 seconds and 0.0001 seconds precompiling for 18 choices
W1221 02:48:36.196000 192962 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1221 02:48:36.196000 192962 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1221 02:48:36.196000 192962 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1221 02:48:36.196000 192962 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1221 02:48:36.196000 192962 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 1 (GPU 1) output:
Process 192963: CUDA_VISIBLE_DEVICES = 1
Process 192963: CUDA available: True
Process 192963: Device count: 1
Process 192963: Current device: 0
Process 192963: Device name: NVIDIA H800
Process 192963 LOCAL_RANK None

Task 1 errors:
AUTOTUNE mm(816x2048, 2048x2048)
  mm 0.0164 ms 100.0% 
  triton_mm_9441 0.0177 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9435 0.0221 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9434 0.0221 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9440 0.0228 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9430 0.0240 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9433 0.0262 ms 62.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9437 0.0270 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9431 0.0292 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9432 0.0331 ms 49.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3468 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(256x1152, 1152x1152)
  mm 0.0087 ms 100.0% 
  triton_mm_72 0.0093 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_76 0.0101 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_68 0.0116 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_82 0.0125 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_71 0.0129 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_75 0.0133 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_65 0.0137 ms 63.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_67 0.0142 ms 61.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_81 0.0143 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2633 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE convolution(1x3x224x224, 1152x3x14x14)
  convolution 0.0640 ms 100.0% 
  triton_convolution2d_4 0.1530 ms 41.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_3 0.2003 ms 31.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_5 0.2141 ms 29.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_6 0.2175 ms 29.4% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_2 0.3448 ms 18.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=1, num_warps=8
  triton_convolution2d_1 0.4073 ms 15.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_0 0.7437 ms 8.6% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.7831 seconds and 0.0001 seconds precompiling for 8 choices
AUTOTUNE mm(256x4304, 4304x1152)
  mm 0.0147 ms 100.0% 
  triton_mm_110 0.0178 ms 82.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_114 0.0219 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_106 0.0252 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_120 0.0293 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_109 0.0355 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_113 0.0367 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_105 0.0379 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_103 0.0381 ms 38.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_119 0.0391 ms 37.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3502 seconds and 0.0001 seconds precompiling for 20 choices
AUTOTUNE mm(256x4304, 4304x1152)
  mm 0.0146 ms 100.0% 
  triton_mm_3651 0.0181 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_3655 0.0221 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_3647 0.0258 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_3661 0.0298 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_3650 0.0352 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_3654 0.0364 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_3646 0.0384 ms 38.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_3644 0.0386 ms 37.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_3660 0.0391 ms 37.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3513 seconds and 0.0000 seconds precompiling for 20 choices
AUTOTUNE addmm(256x2048, 256x1152, 1152x2048)
  bias_addmm 0.0096 ms 100.0% 
  triton_mm_9263 0.0102 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9267 0.0113 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9259 0.0131 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9273 0.0134 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  addmm 0.0137 ms 70.0% 
  triton_mm_9262 0.0142 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9266 0.0147 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9258 0.0149 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9256 0.0157 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2812 seconds and 0.0001 seconds precompiling for 21 choices
AUTOTUNE bmm(1x128x1, 1x1x816)
  triton_bmm_9332 0.0053 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_9331 0.0055 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_bmm_9334 0.0055 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9335 0.0056 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_9333 0.0057 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9336 0.0057 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_9338 0.0057 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_9337 0.0059 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9342 0.0064 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9340 0.0065 ms 81.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2010 seconds and 0.0001 seconds precompiling for 17 choices
AUTOTUNE mm(816x2048, 2048x256)
  mm 0.0099 ms 100.0% 
  triton_mm_9351 0.0104 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9355 0.0109 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9359 0.0126 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9365 0.0164 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9350 0.0172 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9354 0.0174 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9349 0.0180 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9348 0.0181 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_9358 0.0186 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2819 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE bmm(8x816x256, 8x256x816)
  bmm 0.0137 ms 100.0% 
  triton_bmm_9382 0.0138 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9377 0.0151 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9383 0.0165 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9376 0.0170 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9380 0.0170 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9384 0.0171 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9379 0.0173 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9373 0.0182 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9375 0.0183 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2734 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE bmm(8x816x816, 8x816x256)
  bmm 0.0124 ms 100.0% 
  triton_bmm_9422 0.0139 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9415 0.0146 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9421 0.0151 ms 82.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9414 0.0156 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9411 0.0158 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9418 0.0164 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9416 0.0170 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_9413 0.0179 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9417 0.0191 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2687 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(816x16384, 16384x2048)
  mm 0.0867 ms 100.0% 
  triton_mm_9498 0.0982 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9491 0.1256 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9497 0.1300 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9492 0.1422 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9490 0.1576 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9488 0.1655 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9494 0.1691 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9487 0.1855 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9489 0.2092 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6389 seconds and 0.0002 seconds precompiling for 20 choices
E1221 02:07:55.431000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.431000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.431000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.483000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.483000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 327680, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.483000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.530000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.530000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 393216, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.530000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.578000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.578000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.578000 192963 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
AUTOTUNE addmm(50x1024, 50x1024, 1024x1024)
  bias_addmm 0.0116 ms 100.0% 
  triton_mm_12322 0.0128 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12321 0.0140 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12326 0.0151 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12333 0.0151 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12320 0.0152 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12330 0.0161 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  addmm 0.0161 ms 72.2% 
  triton_mm_12329 0.0167 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12332 0.0182 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2282 seconds and 0.0002 seconds precompiling for 19 choices
AUTOTUNE addmm(1x1024, 1x32, 32x1024)
  triton_mm_12283 0.0056 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12278 0.0057 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12275 0.0057 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12279 0.0057 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12276 0.0061 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2
  triton_mm_12281 0.0061 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12274 0.0062 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2
  triton_mm_12280 0.0062 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12285 0.0062 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12273 0.0063 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.1835 seconds and 0.0001 seconds precompiling for 15 choices
AUTOTUNE addmm(50x1024, 50x32, 32x1024)
  triton_mm_12287 0.0055 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12288 0.0058 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12289 0.0060 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12292 0.0061 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12293 0.0061 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12299 0.0061 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12294 0.0063 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12298 0.0063 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12290 0.0064 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12286 0.0067 ms 82.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.2125 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE mm(51x1024, 1024x2048)
  mm 0.0085 ms 100.0% 
  triton_mm_12340 0.0089 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12344 0.0092 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12348 0.0102 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12352 0.0107 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12339 0.0120 ms 70.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12343 0.0123 ms 69.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12338 0.0126 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12347 0.0127 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12337 0.0128 ms 66.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2309 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(1x128x1, 1x1x51)
  triton_bmm_12354 0.0052 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12353 0.0053 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_bmm_12355 0.0053 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12357 0.0053 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12359 0.0056 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12363 0.0056 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12356 0.0056 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12361 0.0057 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12358 0.0057 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12362 0.0058 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.1879 seconds and 0.0002 seconds precompiling for 16 choices
AUTOTUNE mm(51x1024, 1024x256)
  mm 0.0073 ms 100.0% 
  triton_mm_12372 0.0076 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12376 0.0079 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12380 0.0091 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12384 0.0098 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12371 0.0107 ms 68.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12375 0.0111 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12370 0.0113 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12379 0.0113 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12369 0.0117 ms 62.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2294 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(8x51x256, 8x256x867)
  triton_bmm_12392 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12393 0.0077 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12399 0.0079 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_12396 0.0081 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12401 0.0081 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12397 0.0083 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12388 0.0084 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12386 0.0085 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12387 0.0085 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12395 0.0086 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2156 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(8x56x872, 8x872x256)
  bmm 0.0086 ms 100.0% 
  triton_bmm_12423 0.0088 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12427 0.0096 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12431 0.0103 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12422 0.0113 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12435 0.0113 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12421 0.0115 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12426 0.0117 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12430 0.0125 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12433 0.0129 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2312 seconds and 0.0001 seconds precompiling for 18 choices
AUTOTUNE mm(51x4096, 4096x1024)
  mm 0.0138 ms 100.0% 
  triton_mm_12491 0.0142 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12495 0.0153 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12499 0.0221 ms 62.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12503 0.0242 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12490 0.0302 ms 45.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12494 0.0317 ms 43.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12489 0.0318 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12498 0.0318 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12488 0.0343 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2953 seconds and 0.0001 seconds precompiling for 18 choices
W1221 02:54:00.870000 192963 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1221 02:54:00.870000 192963 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1221 02:54:00.870000 192963 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1221 02:54:00.870000 192963 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1221 02:54:00.870000 192963 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 2 (GPU 2) output:
Process 192965: CUDA_VISIBLE_DEVICES = 2
Process 192965: CUDA available: True
Process 192965: Device count: 1
Process 192965: Current device: 0
Process 192965: Device name: NVIDIA H800
Process 192965 LOCAL_RANK None

Task 2 errors:
AUTOTUNE mm(816x2048, 2048x16384)
  mm 0.0776 ms 100.0% 
  triton_mm_9478 0.1026 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9479 0.1039 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9477 0.1284 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9472 0.1289 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9473 0.1299 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9468 0.1676 ms 46.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9470 0.1682 ms 46.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9471 0.1695 ms 45.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9475 0.1759 ms 44.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.2312 seconds and 62.2816 seconds precompiling for 20 choices
AUTOTUNE mm(256x1152, 1152x4304)
  mm 0.0124 ms 100.0% 
  triton_mm_101 0.0137 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_94 0.0154 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_90 0.0156 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_95 0.0162 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_100 0.0163 ms 75.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_91 0.0176 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_97 0.0177 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_93 0.0178 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_96 0.0214 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2857 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(816x2048, 2048x2048)
  mm 0.0159 ms 100.0% 
  triton_mm_9441 0.0177 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9435 0.0220 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9434 0.0220 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9440 0.0230 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9430 0.0237 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9433 0.0264 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9437 0.0268 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9431 0.0295 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9432 0.0331 ms 48.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3105 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE mm(256x1152, 1152x1152)
  mm 0.0089 ms 100.0% 
  triton_mm_72 0.0093 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_76 0.0104 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_68 0.0117 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_82 0.0123 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_71 0.0128 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_75 0.0132 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_65 0.0138 ms 64.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_67 0.0143 ms 62.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_81 0.0143 ms 62.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2643 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE convolution(1x3x224x224, 1152x3x14x14)
  convolution 0.0639 ms 100.0% 
  triton_convolution2d_4 0.1535 ms 41.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_3 0.2005 ms 31.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_5 0.2122 ms 30.1% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_6 0.2193 ms 29.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_2 0.3502 ms 18.3% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=1, num_warps=8
  triton_convolution2d_1 0.4205 ms 15.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_0 0.7380 ms 8.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4512 seconds and 0.0001 seconds precompiling for 8 choices
AUTOTUNE mm(256x4304, 4304x1152)
  mm 0.0149 ms 100.0% 
  triton_mm_110 0.0178 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_114 0.0219 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_106 0.0252 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_120 0.0296 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_109 0.0356 ms 41.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_113 0.0367 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_105 0.0380 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_103 0.0382 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_119 0.0395 ms 37.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3542 seconds and 0.0001 seconds precompiling for 20 choices
AUTOTUNE addmm(256x2048, 256x1152, 1152x2048)
  bias_addmm 0.0100 ms 100.0% 
  triton_mm_9263 0.0105 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9267 0.0116 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9259 0.0136 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9273 0.0137 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  addmm 0.0139 ms 71.9% 
  triton_mm_9262 0.0139 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9266 0.0147 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9258 0.0147 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9256 0.0158 ms 63.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2817 seconds and 0.0001 seconds precompiling for 21 choices
AUTOTUNE bmm(1x128x1, 1x1x816)
  triton_bmm_9331 0.0052 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_bmm_9333 0.0054 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9335 0.0054 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_9332 0.0055 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_9334 0.0057 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9337 0.0057 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9336 0.0058 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_9338 0.0059 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_9341 0.0065 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_9339 0.0065 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2008 seconds and 0.0001 seconds precompiling for 17 choices
AUTOTUNE mm(816x2048, 2048x256)
  mm 0.0102 ms 100.0% 
  triton_mm_9351 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9355 0.0106 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9359 0.0122 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9365 0.0160 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9350 0.0171 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9354 0.0174 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9349 0.0175 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9348 0.0183 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_9358 0.0185 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2782 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE bmm(8x816x256, 8x256x816)
  bmm 0.0138 ms 100.0% 
  triton_bmm_9382 0.0140 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9377 0.0149 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9383 0.0164 ms 83.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9380 0.0172 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9376 0.0172 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9379 0.0172 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9384 0.0175 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9373 0.0180 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9375 0.0181 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2731 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE bmm(8x816x816, 8x816x256)
  bmm 0.0128 ms 100.0% 
  triton_bmm_9422 0.0136 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9415 0.0146 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9421 0.0152 ms 84.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9414 0.0156 ms 81.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9411 0.0158 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9418 0.0160 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9416 0.0169 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_9413 0.0183 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9420 0.0191 ms 66.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2700 seconds and 0.0001 seconds precompiling for 20 choices
AUTOTUNE mm(816x16384, 16384x2048)
  mm 0.0882 ms 100.0% 
  triton_mm_9498 0.0979 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9491 0.1251 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9497 0.1297 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9492 0.1422 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9490 0.1592 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9488 0.1658 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9494 0.1665 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9487 0.1840 ms 47.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9489 0.2108 ms 41.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6430 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE addmm(50x1024, 50x32, 32x1024)
  triton_mm_12287 0.0056 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12289 0.0060 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12288 0.0060 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12299 0.0060 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12293 0.0061 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12298 0.0063 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12292 0.0063 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12294 0.0064 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12286 0.0068 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_12290 0.0068 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2105 seconds and 0.0001 seconds precompiling for 18 choices
AUTOTUNE mm(51x1024, 1024x2048)
  mm 0.0086 ms 100.0% 
  triton_mm_12340 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12344 0.0092 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12348 0.0102 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12352 0.0109 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12339 0.0121 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12343 0.0123 ms 69.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12347 0.0128 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12338 0.0128 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12337 0.0129 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2292 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(1x128x1, 1x1x51)
  triton_bmm_12353 0.0053 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_bmm_12355 0.0054 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12354 0.0054 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12357 0.0054 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12359 0.0056 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12361 0.0057 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12363 0.0057 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12356 0.0057 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12358 0.0059 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12360 0.0059 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.1881 seconds and 0.0002 seconds precompiling for 16 choices
AUTOTUNE mm(51x1024, 1024x256)
  mm 0.0073 ms 100.0% 
  triton_mm_12372 0.0077 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12376 0.0080 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12380 0.0090 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12384 0.0098 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12371 0.0106 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12375 0.0110 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12379 0.0112 ms 65.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12370 0.0112 ms 65.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12369 0.0115 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2270 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(8x51x256, 8x256x867)
  triton_bmm_12393 0.0077 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12392 0.0078 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12399 0.0078 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_12401 0.0080 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12396 0.0082 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12397 0.0084 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12387 0.0084 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12386 0.0085 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12388 0.0085 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12395 0.0087 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2153 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(8x56x872, 8x872x256)
  bmm 0.0087 ms 100.0% 
  triton_bmm_12423 0.0091 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12427 0.0096 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12431 0.0102 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12435 0.0112 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12422 0.0113 ms 77.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12421 0.0115 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12426 0.0116 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12430 0.0123 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12433 0.0129 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2307 seconds and 0.0001 seconds precompiling for 18 choices
AUTOTUNE mm(51x4096, 4096x1024)
  mm 0.0138 ms 100.0% 
  triton_mm_12491 0.0143 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12495 0.0154 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12499 0.0220 ms 62.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12503 0.0243 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12490 0.0305 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12489 0.0318 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12494 0.0321 ms 43.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12498 0.0322 ms 42.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12488 0.0346 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2954 seconds and 0.0001 seconds precompiling for 18 choices
W1221 02:46:56.163000 192965 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1221 02:46:56.163000 192965 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1221 02:46:56.163000 192965 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1221 02:46:56.163000 192965 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1221 02:46:56.163000 192965 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 3 (GPU 3) output:
Process 192966: CUDA_VISIBLE_DEVICES = 3
Process 192966: CUDA available: True
Process 192966: Device count: 1
Process 192966: Current device: 0
Process 192966: Device name: NVIDIA H800
Process 192966 LOCAL_RANK None

Task 3 errors:
AUTOTUNE convolution(1x3x224x224, 1152x3x14x14)
  convolution 0.0649 ms 100.0% 
  triton_convolution2d_4 0.1525 ms 42.5% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_3 0.1976 ms 32.8% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=128, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_5 0.2145 ms 30.2% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_6 0.2165 ms 30.0% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=8
  triton_convolution2d_2 0.3477 ms 18.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=1024, BLOCK_N=16, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=1, num_warps=8
  triton_convolution2d_1 0.4080 ms 15.9% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=256, BLOCK_N=64, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
  triton_convolution2d_0 0.7456 ms 8.7% ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=256, GROUPS=1, KERNEL_H=14, KERNEL_W=14, PADDING_H=0, PADDING_W=0, STRIDE_H=14, STRIDE_W=14, UNROLL=False, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.7657 seconds and 0.0001 seconds precompiling for 8 choices
AUTOTUNE mm(256x4304, 4304x1152)
  mm 0.0148 ms 100.0% 
  triton_mm_110 0.0180 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_114 0.0219 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_106 0.0254 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_120 0.0297 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_109 0.0356 ms 41.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_113 0.0365 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_105 0.0380 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_103 0.0383 ms 38.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_119 0.0392 ms 37.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3797 seconds and 0.0001 seconds precompiling for 20 choices
AUTOTUNE mm(256x4304, 4304x1152)
  mm 0.0147 ms 100.0% 
  triton_mm_2504 0.0181 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_2508 0.0223 ms 65.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_2500 0.0258 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_2514 0.0299 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_2503 0.0354 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_2507 0.0365 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_2497 0.0381 ms 38.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_2499 0.0384 ms 38.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_2513 0.0390 ms 37.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3468 seconds and 0.0000 seconds precompiling for 20 choices
AUTOTUNE addmm(256x2048, 256x1152, 1152x2048)
  bias_addmm 0.0099 ms 100.0% 
  triton_mm_9263 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9267 0.0114 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9259 0.0132 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9273 0.0136 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  addmm 0.0139 ms 71.0% 
  triton_mm_9262 0.0143 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9266 0.0147 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9258 0.0151 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9256 0.0157 ms 62.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2821 seconds and 0.0001 seconds precompiling for 21 choices
AUTOTUNE bmm(1x128x1, 1x1x816)
  triton_bmm_9332 0.0054 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_9331 0.0056 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_bmm_9334 0.0057 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9333 0.0057 ms 93.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9335 0.0059 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_9336 0.0060 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_9338 0.0060 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_9337 0.0060 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9342 0.0064 ms 83.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9340 0.0067 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2012 seconds and 0.0001 seconds precompiling for 17 choices
AUTOTUNE mm(816x2048, 2048x256)
  mm 0.0101 ms 100.0% 
  triton_mm_9351 0.0105 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9355 0.0109 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9359 0.0127 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9365 0.0163 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9350 0.0170 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9354 0.0176 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9348 0.0183 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_9349 0.0183 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9358 0.0188 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2804 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE bmm(8x816x256, 8x256x816)
  bmm 0.0136 ms 100.0% 
  triton_bmm_9382 0.0140 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9377 0.0152 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9383 0.0165 ms 82.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9379 0.0169 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9380 0.0170 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9384 0.0172 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9376 0.0173 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9373 0.0183 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9375 0.0185 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2746 seconds and 0.0002 seconds precompiling for 20 choices
AUTOTUNE bmm(8x816x816, 8x816x256)
  bmm 0.0125 ms 100.0% 
  triton_bmm_9422 0.0139 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_9415 0.0145 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9421 0.0149 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9414 0.0157 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9411 0.0158 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_9418 0.0164 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_9416 0.0173 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_9413 0.0182 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_9417 0.0191 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2674 seconds and 0.0001 seconds precompiling for 20 choices
AUTOTUNE mm(816x16384, 16384x2048)
  mm 0.0887 ms 100.0% 
  triton_mm_9498 0.0978 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_9491 0.1255 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9497 0.1309 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9492 0.1419 ms 62.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_9490 0.1598 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9488 0.1673 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_9494 0.1675 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_9487 0.1888 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_9489 0.2110 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6394 seconds and 0.0002 seconds precompiling for 20 choices
E1221 02:07:24.891000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:24.891000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:24.891000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
AUTOTUNE addmm(50x32, 50x1024, 1024x32)
  triton_mm_15116 0.0095 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_15118 0.0102 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  bias_addmm 0.0112 ms 84.9% 
  triton_mm_15107 0.0116 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_15108 0.0117 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_15115 0.0122 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_15112 0.0126 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_15114 0.0132 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_15106 0.0138 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  addmm 0.0139 ms 68.7% 
SingleProcess AUTOTUNE benchmarking takes 0.2165 seconds and 0.9446 seconds precompiling for 16 choices
AUTOTUNE mm(51x2048, 2048x1024)
  mm 0.0096 ms 100.0% 
  triton_mm_12440 0.0100 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12444 0.0106 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12448 0.0124 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12452 0.0138 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12439 0.0172 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12443 0.0179 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12438 0.0180 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12437 0.0187 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12447 0.0190 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2553 seconds and 28.2496 seconds precompiling for 18 choices
E1221 02:07:55.451000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.451000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.451000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.518000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.518000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 327680, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.518000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.573000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.573000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 393216, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.573000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
E1221 02:07:55.621000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Runtime error during autotuning: 
E1221 02:07:55.621000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 245760, Hardware limit: 232448. Reducing block sizes or `num_stages` may help.. 
E1221 02:07:55.621000 192966 torch/_inductor/select_algorithm.py:2100] [7/0] Ignoring this choice.
AUTOTUNE addmm(50x1024, 50x1024, 1024x1024)
  bias_addmm 0.0117 ms 100.0% 
  triton_mm_12322 0.0134 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12321 0.0141 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12333 0.0150 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12326 0.0151 ms 77.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12320 0.0156 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12330 0.0160 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  addmm 0.0164 ms 71.2% 
  triton_mm_12329 0.0166 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12332 0.0180 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2668 seconds and 0.0002 seconds precompiling for 19 choices
AUTOTUNE addmm(1x1024, 1x32, 32x1024)
  triton_mm_12275 0.0058 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12279 0.0058 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12283 0.0058 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12278 0.0059 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12281 0.0060 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12285 0.0061 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12276 0.0062 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=2
  triton_mm_12273 0.0063 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
  triton_mm_12274 0.0063 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=2
  triton_mm_12284 0.0063 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.1860 seconds and 0.0001 seconds precompiling for 15 choices
AUTOTUNE addmm(50x1024, 50x32, 32x1024)
  triton_mm_12287 0.0057 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12288 0.0060 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12289 0.0060 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12299 0.0061 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_12293 0.0061 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12292 0.0062 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12294 0.0062 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12298 0.0063 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12290 0.0066 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12286 0.0067 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=1, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.2148 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE mm(51x1024, 1024x2048)
  mm 0.0086 ms 100.0% 
  triton_mm_12340 0.0089 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12344 0.0090 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12348 0.0103 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12352 0.0109 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12339 0.0121 ms 70.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12343 0.0124 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12338 0.0126 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12347 0.0126 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12337 0.0127 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2340 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(1x128x1, 1x1x51)
  triton_bmm_12353 0.0052 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2
  triton_bmm_12354 0.0053 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12355 0.0053 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12357 0.0054 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12356 0.0056 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12359 0.0056 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12361 0.0056 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12363 0.0056 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12358 0.0058 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12362 0.0059 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.1908 seconds and 0.0002 seconds precompiling for 16 choices
AUTOTUNE mm(51x1024, 1024x256)
  mm 0.0074 ms 100.0% 
  triton_mm_12372 0.0075 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12376 0.0080 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12380 0.0092 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12384 0.0098 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12371 0.0107 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12375 0.0109 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12369 0.0113 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_12379 0.0113 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12370 0.0115 ms 64.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2292 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(8x51x256, 8x256x867)
  triton_bmm_12392 0.0078 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12393 0.0078 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12399 0.0081 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_bmm_12396 0.0081 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12401 0.0081 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12397 0.0084 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12386 0.0086 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_bmm_12387 0.0086 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12388 0.0086 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12395 0.0087 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2205 seconds and 0.0002 seconds precompiling for 18 choices
AUTOTUNE bmm(8x56x872, 8x872x256)
  bmm 0.0087 ms 100.0% 
  triton_bmm_12423 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12427 0.0096 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=4
  triton_bmm_12431 0.0104 ms 83.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=4
  triton_bmm_12435 0.0114 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12422 0.0115 ms 76.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12421 0.0117 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8
  triton_bmm_12426 0.0117 ms 74.1% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8
  triton_bmm_12430 0.0125 ms 69.7% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4
  triton_bmm_12433 0.0130 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2347 seconds and 0.0001 seconds precompiling for 18 choices
AUTOTUNE mm(51x4096, 4096x1024)
  mm 0.0140 ms 100.0% 
  triton_mm_12491 0.0143 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12495 0.0153 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_12499 0.0221 ms 63.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=4
  triton_mm_12503 0.0242 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12490 0.0302 ms 46.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12498 0.0316 ms 44.4% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_12489 0.0319 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_12494 0.0319 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_12488 0.0347 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=True, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.2957 seconds and 0.0001 seconds precompiling for 18 choices
W1221 02:51:21.234000 192966 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1221 02:51:21.234000 192966 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1221 02:51:21.234000 192966 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1221 02:51:21.234000 192966 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1221 02:51:21.234000 192966 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 4 (GPU 4) output:
Process 192967: CUDA_VISIBLE_DEVICES = 4
Process 192967: CUDA available: False
Process 192967: Device count: 0
Process 192967 LOCAL_RANK None

Task 4 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 5 (GPU 5) output:
Process 192968: CUDA_VISIBLE_DEVICES = 5
Process 192968: CUDA available: False
Process 192968: Device count: 0
Process 192968 LOCAL_RANK None

Task 5 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 6 (GPU 6) output:
Process 192969: CUDA_VISIBLE_DEVICES = 6
Process 192969: CUDA available: False
Process 192969: Device count: 0
Process 192969 LOCAL_RANK None

Task 6 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 7 (GPU 7) output:
Process 192970: CUDA_VISIBLE_DEVICES = 7
Process 192970: CUDA available: False
Process 192970: Device count: 0
Process 192970 LOCAL_RANK None

Task 7 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Process 191938: Error in task 4: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-21 01:52:23_gpu_4_result.pkl'
Process 191938: Error in task 5: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-21 01:52:23_gpu_5_result.pkl'
Process 191938: Error in task 6: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-21 01:52:23_gpu_6_result.pkl'
Process 191938: Error in task 7: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-21 01:52:23_gpu_7_result.pkl'
Total time: 5966.04322719574
Length of results: 64
Success rate: 28.1%
Task 0 (GPU 0) output:
Process 392802: CUDA_VISIBLE_DEVICES = 0
Process 392802: CUDA available: True
Process 392802: Device count: 1
Process 392802: Current device: 0
Process 392802: Device name: NVIDIA H800
Process 392802 LOCAL_RANK None

Task 0 errors:
W1221 03:24:44.521000 392802 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1221 03:24:44.521000 392802 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1221 03:24:44.521000 392802 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1221 03:24:44.521000 392802 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1221 03:24:44.521000 392802 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 1 (GPU 1) output:
Process 392803: CUDA_VISIBLE_DEVICES = 1
Process 392803: CUDA available: True
Process 392803: Device count: 1
Process 392803: Current device: 0
Process 392803: Device name: NVIDIA H800
Process 392803 LOCAL_RANK None

Task 1 errors:
W1221 03:26:19.151000 392803 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1221 03:26:19.151000 392803 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1221 03:26:19.151000 392803 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1221 03:26:19.151000 392803 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1221 03:26:19.151000 392803 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 2 (GPU 2) output:
Process 392804: CUDA_VISIBLE_DEVICES = 2
Process 392804: CUDA available: True
Process 392804: Device count: 1
Process 392804: Current device: 0
Process 392804: Device name: NVIDIA H800
Process 392804 LOCAL_RANK None

Task 2 errors:
W1221 03:27:56.922000 392804 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1221 03:27:56.922000 392804 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1221 03:27:56.922000 392804 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1221 03:27:56.922000 392804 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1221 03:27:56.922000 392804 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 3 (GPU 3) output:
Process 392805: CUDA_VISIBLE_DEVICES = 3
Process 392805: CUDA available: True
Process 392805: Device count: 1
Process 392805: Current device: 0
Process 392805: Device name: NVIDIA H800
Process 392805 LOCAL_RANK None

Task 3 errors:
W1221 03:28:18.918000 392805 torch/_dynamo/convert_frame.py:964] [0/8] torch._dynamo hit config.recompile_limit (8)
W1221 03:28:18.918000 392805 torch/_dynamo/convert_frame.py:964] [0/8]    function: 'inner' (/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:68)
W1221 03:28:18.918000 392805 torch/_dynamo/convert_frame.py:964] [0/8]    last reason: 0/0: Cache line invalidated because L['fn'] got deallocated
W1221 03:28:18.918000 392805 torch/_dynamo/convert_frame.py:964] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W1221 03:28:18.918000 392805 torch/_dynamo/convert_frame.py:964] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.

Task 4 (GPU 4) output:
Process 392806: CUDA_VISIBLE_DEVICES = 4
Process 392806: CUDA available: False
Process 392806: Device count: 0
Process 392806 LOCAL_RANK None

Task 4 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 5 (GPU 5) output:
Process 392807: CUDA_VISIBLE_DEVICES = 5
Process 392807: CUDA available: False
Process 392807: Device count: 0
Process 392807 LOCAL_RANK None

Task 5 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 6 (GPU 6) output:
Process 392809: CUDA_VISIBLE_DEVICES = 6
Process 392809: CUDA available: False
Process 392809: Device count: 0
Process 392809 LOCAL_RANK None

Task 6 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Task 7 (GPU 7) output:
Process 392810: CUDA_VISIBLE_DEVICES = 7
Process 392810: CUDA available: False
Process 392810: Device count: 0
Process 392810 LOCAL_RANK None

Task 7 errors:
Traceback (most recent call last):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/robot/planner.py", line 18, in <module>
    from curobo.wrap.reacher.motion_gen import (
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 1425, in <module>
    class MotionGen(MotionGenConfig):
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/wrap/reacher/motion_gen.py", line 4203, in MotionGen
    grasp_approach_offset: Pose = Pose.from_list([0, 0, -0.15, 1, 0, 0, 0]),
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/envs/curobo/src/curobo/types/math.py", line 215, in from_list
    position = torch.as_tensor(
               ^^^^^^^^^^^^^^^^
  File "/project/peilab/yanzhengyang/RoboTwin/policy/yzy_openpi/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py", line 372, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
No Task: beat_block_hammer

Process 392232: Error in task 4: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-21 02:46:39_gpu_4_result.pkl'
Process 392232: Error in task 5: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-21 02:46:39_gpu_5_result.pkl'
Process 392232: Error in task 6: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-21 02:46:39_gpu_6_result.pkl'
Process 392232: Error in task 7: [Errno 2] No such file or directory: '/project/peilab/yanzhengyang/RoboTwin/tmp/2025-12-21 02:46:39_gpu_7_result.pkl'
Total time: 4647.111441612244
Length of results: 64
Success rate: 25.0%
